{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "도커 pytorch/pytorch:1.13.1-cuda11.6-cudnn8-devel 를 사용하여.\n",
    "pytorch 및 cuda 설치과정 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토크나이저와 모델 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T13:19:04.089415400Z",
     "start_time": "2023-07-27T13:19:03.913551500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b1b245301e4a7fa7813ef22e792c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd49aeec7704d2da87d7fde9aacdd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('443'), PosixPath('tcp'), PosixPath('//10.88.0.1')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('tcp'), PosixPath('8887'), PosixPath('//10.88.7.255')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8888'), PosixPath('tcp'), PosixPath('//10.88.7.255')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/aiffel/storage/package')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "ERROR: /opt/conda/bin/python: undefined symbol: cudaRuntimeGetVersion\n",
      "CUDA SETUP: libcudart.so path is None\n",
      "CUDA SETUP: Is seems that your cuda installation is not in your path. See https://github.com/TimDettmers/bitsandbytes/issues/85 for more information.\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 00\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3deedf2fbe4bd5bda3e4b6496bc2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': 1024,\n",
       " 'gpt2-medium': 1024,\n",
       " 'gpt2-large': 1024,\n",
       " 'gpt2-xl': 1024,\n",
       " 'distilgpt2': 1024}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 토크나이저 테스트\n",
    "input_txt = \"바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\"\n",
    "\n",
    "tokens = tokenizer(input_txt).tokens()\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kogpt-2_tokens</th>\n",
       "      <td>▁바람</td>\n",
       "      <td>도</td>\n",
       "      <td>▁없는</td>\n",
       "      <td>▁공중에</td>\n",
       "      <td>▁수직</td>\n",
       "      <td>의</td>\n",
       "      <td>▁파</td>\n",
       "      <td>문을</td>\n",
       "      <td>▁내</td>\n",
       "      <td>이며</td>\n",
       "      <td>▁고</td>\n",
       "      <td>요</td>\n",
       "      <td>히</td>\n",
       "      <td>▁떨어지는</td>\n",
       "      <td>▁오동</td>\n",
       "      <td>잎은</td>\n",
       "      <td>▁누</td>\n",
       "      <td>구의</td>\n",
       "      <td>▁발자</td>\n",
       "      <td>취</td>\n",
       "      <td>▁입</td>\n",
       "      <td>니까</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input_IDs</th>\n",
       "      <td>10891</td>\n",
       "      <td>7235</td>\n",
       "      <td>9712</td>\n",
       "      <td>49207</td>\n",
       "      <td>14438</td>\n",
       "      <td>8143</td>\n",
       "      <td>9203</td>\n",
       "      <td>9941</td>\n",
       "      <td>9094</td>\n",
       "      <td>9639</td>\n",
       "      <td>9065</td>\n",
       "      <td>8084</td>\n",
       "      <td>8811</td>\n",
       "      <td>21215</td>\n",
       "      <td>34769</td>\n",
       "      <td>19985</td>\n",
       "      <td>9669</td>\n",
       "      <td>10139</td>\n",
       "      <td>21626</td>\n",
       "      <td>8408</td>\n",
       "      <td>9241</td>\n",
       "      <td>23775</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0     1     2      3      4     5     6     7     8     9   \\\n",
       "kogpt-2_tokens    ▁바람     도   ▁없는   ▁공중에    ▁수직     의    ▁파    문을    ▁내    이며   \n",
       "Input_IDs       10891  7235  9712  49207  14438  8143  9203  9941  9094  9639   \n",
       "\n",
       "                  10    11    12     13     14     15    16     17     18  \\\n",
       "kogpt-2_tokens    ▁고     요     히  ▁떨어지는    ▁오동     잎은    ▁누     구의    ▁발자   \n",
       "Input_IDs       9065  8084  8811  21215  34769  19985  9669  10139  21626   \n",
       "\n",
       "                  19    20     21   22  \n",
       "kogpt-2_tokens     취    ▁입     니까    .  \n",
       "Input_IDs       8408  9241  23775  389  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 40\n",
    "pd.options.display.max_rows = 60\n",
    "df = pd.DataFrame([tokens, input_ids[0]], index=[\"kogpt-2_tokens\", \"Input_IDs\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.'\n",
      "\"그렇다면 그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리\n"
     ]
    }
   ],
   "source": [
    "# 디코더 확인\n",
    "\n",
    "max_length=128\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리디 서치 디코더라 시퀀스가 반복되는 현상 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.'\n",
      "\"그렇지 않습니다.\"\n",
      "\"어떻게 된 일입니까?\"\n",
      "그녀는 고개를 갸웃거렸다.\n",
      "\"아니, 그게 무슨 말씀이신지 모르겠습니다만.\"\n",
      "\"무슨 말씀인지 알 수가 없군요.\"\n",
      "아무런 대답도 하지 않은 채 그녀는 고개를 끄덕였다.\n",
      "\"그래, 알았어.\"\n",
      "그녀의 눈에서 눈물이 주르륵 흘러내렸다.\n",
      "그녀가 다시 입을 열었다.\n",
      "\"정말 죄송합니다, 고마워요, 고맙습니다\"\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "# 빔서치 디코딩에 n-gram 패널티 부과\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=10, no_repeat_ngram_size=2, do_sample=False)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까. 어머님께서 댁으로 모시고 가시고 계시다가 이게 언제 날벼락이 되겠습니까? 제가 그 얘기를 안 한 게 지금 이 년 전 얘기죠. 그게 뭐냐 하면은 그 사십구일 때 저희 어머니가 저를 불러오셨던 거예요. 저는 저기 저하고 같이 자고 갔거든요. 근데 그 집에 오시는 길에 저한테 뭐가 잘못됐느냐고 묻고는 뭐라고 답변해 달라고 그랬더니 어머니께서는 대답을 하지 않으\n"
     ]
    }
   ],
   "source": [
    "# 샘플링 기법 추가 - top_k\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, temperature=2.0, top_k=50)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\"\n",
      "\"그렇지. 그건 그렇고.\"\n",
      "\"아니야. 그런 건 아니잖아. 그러니까 그게 뭐야?\"\n",
      "그녀는 고개를 끄덕였다.\n",
      "\"글쎄, 그거야 나도 알고 있어.\"\n",
      "그녀가 고개를 갸웃거렸다.\n",
      "\"그래. 난 그걸 몰라서 그랬어.\"\n",
      "그러자 그녀의 눈에서 눈물이 흘러내렸다.\n",
      "그녀의 입에서 눈물방울이 뚝뚝 떨어지기 시작했다.\n",
      "\"나도 알았어. 괜찮을 거라고 생각했었거든.\n"
     ]
    }
   ],
   "source": [
    "# top_p 기법 사용\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, top_p=0.90)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최선의 디코딩 방법을 찾기 위해선\n",
    "빔사이즈와 n-gram 패널티, temperature와 샘플링 인자로\n",
    "조합할 수 있는 최선의 값을 찾아보는 실험이 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLHF 도입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '불고기용 고기 한우에요?',\n",
       "  'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\",\n",
       "  'tokens': 193},\n",
       " {'prompt': '쓰던 앱이 유료로 전환됐어',\n",
       "  'completion': \"'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\",\n",
       "  'tokens': 288},\n",
       " {'prompt': '여친이랑 다툼',\n",
       "  'completion': \"'저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하지만 다투게 되면 상대방의 말을 경청하는 태도와 서로의 감정을 존중하는 것이 중요합니다. 논리적이고 차분한 대화를 통해 문제를 해결해보세요. 그리고 서로를 이해하고 서로의 의견을 수용하는 것이 중요합니다.\",\n",
       "  'tokens': 153}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "data_path_1_SFT = 'kochatgpt_1_SFT.jsonl' \n",
    "with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?',\n",
       "  'completion_0': 'Allow me to answer your question. I know that you are curious about me.',\n",
       "  'completion_1': '번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.',\n",
       "  'completion_2': '라이언에게 말했다.',\n",
       "  'ranking': [2, 1, 0]},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?',\n",
       "  'completion_0': '개포주공아파트는 다섯 단지로 이루어져 있습니다.',\n",
       "  'completion_1': '이날 목송에서 구글상위노',\n",
       "  'completion_2': '개포주공아파트는 총 27개 단지로 이루어져 있습니다.',\n",
       "  'ranking': [2, 0, 1]},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?',\n",
       "  'completion_0': 'The diameter of the Metallic domain is bigger than the Hyperonic domain.',\n",
       "  'completion_1': '이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가 그 발언을 문제삼았는지에 따라 답이 다를 수 있습니다.\\\\n\\\\n만약 김영삼 대통령이 후보 시절에 지역표심을 겨냥한 발언을 했다는 가정하에, 그 발언을 문제삼은 후보가 누구였는지를 대답하자면, 그 답은 이화선 당시 민주당 대통령 후보가 될 것입니다. 1992년 총선 때, 김영삼 대선후보는 \"집값이 오른 노량진역 부근의 부동산 가격은 세월호 폭침 후 \\\\\\'강남 도시재생\\\\\\' 일환으로 상승했다\"는 발언을 했습니다. 하지만 이화선 후보는 이 발언을 \"전국적으로 경제적 발전이 이루어지지 않은 지방민의 마음을 멀리해지려는 무례한 발언\"이라고 비판하며 문제삼았습니다.\\\\n\\\\n하지만, 이 질문을 답변하는 데 있어서 보다 명확한 정보가 있으면 답변을 보완할 수 있습니다.',\n",
       "  'completion_2': '김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 추구하고 있는 민주주의 광범위하게 확립과 보수의 사상을 이어가는 데 있어 지역경제 발전과 공공서비스 신속 개선을 위해 합리적인 국가 정책에 따르는 방향성을 제시하고 있습니다.',\n",
       "  'ranking': [1, 2, 0]}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_2_RM = 'kochatgpt_2_RM.jsonl'\n",
    "with open(data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?'},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?'},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_3_PPO = 'kochatgpt_3_PPO.jsonl'\n",
    "with open(data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Fine-Tuning (SFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        data_path_1_SFT = 'kochatgpt_1_SFT.jsonl'\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100) \n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n",
      "         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n",
      "          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n",
      "         7192, 14914,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SFT_dataset(data_path_1_SFT='kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded input:  ### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.</s>\n",
      "Decoded output:  '저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.</s>\n"
     ]
    }
   ],
   "source": [
    "def decode(tokenizer, token_ids):\n",
    "    # Remove padding tokens (-100 value)\n",
    "    token_ids = [token_id for token_id in token_ids if token_id != -100]\n",
    "    decoded_string = tokenizer.decode(token_ids)\n",
    "    return decoded_string\n",
    "\n",
    "# Apply the function to the first example\n",
    "input_example = train_dataset.input_ids[0]\n",
    "label_example = train_dataset.labels[0]\n",
    "\n",
    "print(\"Decoded input: \", decode(tokenizer, input_example))\n",
    "print(\"Decoded output: \", decode(tokenizer, label_example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"aiffel/KoChatGPT/test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 05:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.983300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.776400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.687600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('output_1_SFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 고기를 먹을 수 없습니다. 하지만 일반적으로 불고기는 건강에 좋은 식품으로 알려져 있습니다. 또한, 다양한 종류의 고기가 있으니 취향에 따라 선택하시면 좋을 것 같습니다. 감사합니다. 高さん, \"불고기용\"이라는 말은 일본어로 \"불고기\"라는 뜻\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 42대 부통령직을 수행했습니다.作)作)은 \"리처드 닉슨의 41대 부통령직 수행년도를 말합니다.作)는 \"리처드 닉슨슨의 43대 부통령 후보자\"를 의미합니다.作은 \"리처드닉슨의 47대 부통령후보자\"를\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):'시카고 오 헤어 국제공항은 미국 캘리포니아주 샌프란시스코에 위치해 있습니다. J.R. Martin (Johnson Stephen) J.R. Meyer (James Charles) J.B. Meyer, James Christopher (Johannes Char\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이므로 미세먼지에 대한 정보를 알 수 없습니다. 하지만 미세먼지는 건강에 매우 중요하기 때문에 외출 시 마스크를 착용하거나 실외활동을 자제하는 것이 좋습니다. 또한, 미세먼지가 심한 날에는 야외활동을 자제하고, 대중교통을 이용하거나 대중교통을 이용하는 것이 좋습니다.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n   \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # 메모리 관리를 위해 캐시 청소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/library.py:130: UserWarning: Overriding a previously registered kernel for the same operator and the same dispatch key\n",
      "  operator: aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor\n",
      "    registered at aten/src/ATen/RegisterSchema.cpp:6\n",
      "  dispatch key: Meta\n",
      "  previous kernel: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1053\n",
      "       new kernel: registered at /dev/null:228 (Triggered internally at ../aten/src/ATen/core/dispatch/OperatorEntry.cpp:150.)\n",
      "  self.m.impl(name, dispatch_key, fn)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
     ]
    }
   ],
   "source": [
    "with open('kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairWiseLoss(nn.Module):\n",
    "\n",
    "    def forward(self, chosen_reward: torch.Tensor, reject_reward: torch.Tensor) -> torch.Tensor:\n",
    "        probs = torch.sigmoid(chosen_reward - reject_reward)\n",
    "        log_probs = torch.log(probs)\n",
    "        loss = -log_probs.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_ranking2chosen = []\n",
    "\n",
    "for tmp in list_data_dict:\n",
    "     prompt = tmp['prompt']\n",
    "     ranking = tmp['ranking']\n",
    "\n",
    "     for index in range(1, len(ranking)):\n",
    "         n = ranking[0]\n",
    "         m = ranking[index]\n",
    "\n",
    "\n",
    "         data = {\n",
    "             'prompt': prompt,\n",
    "             'chosen': tmp['completion_{}'.format(n)],\n",
    "             'rejected': tmp['completion_{}'.format(m)]\n",
    "         }\n",
    "\n",
    "         total_data_ranking2chosen.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '멋있게 잊어 주자', 'chosen': '가끔은 일어나서 눈 앞의 것들을 확실하게 정리하는 것도 좋을 때가 있죠. 그렇게 해서 지우는 것이 더 나은 경우도 있으니까요. 그리고 그렇게 해서 잊어버리면 조금 더 나은 상황으로 다가갈 수 있을 거라 생각합니다. 그러니 마음을 비워놓고 새로운 시작을 해보세요. 더 나은 날들이 오길 바랄게요!', 'rejected': '멋있다 공개 개량\\n\\n잊어 개량\\n\\n주자 개량\\n\\n멋있다 공개 개량\\n\\n잊어 개량\\n\\n주자 개량'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1088.64it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 996.09it/s] \n"
     ]
    }
   ],
   "source": [
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "## prompt ##\n",
      "가방 같은 것도 수선해줘요?\n",
      "######################################################################\n",
      "## chosen ##\n",
      "네, 가방도 수선이 가능합니다. 다만 사용하고 있는 가방의 종류와 손상 정도에 따라 수선 방법과 비용이 달라질 수 있습니다. 수선 전에는 꼭 전문가의 상담을 받아보시는 것이 좋습니다.\n",
      "######################################################################\n",
      "## rejected ##\n",
      "다행히 가방을 수선해 드릴 수 있습니다. 다만 보다 정확한 예상 가격을 알기 위해 약간의 정보가 필요합니다. 먼저 가방에 대한 정보를 알려주시면 저희는 가격 협의를 위해 가방을 보고 다시 연락드리겠습니다.\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Train step of epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:01<04:45,  1.15s/it]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:01<04:45,  1.15s/it, loss=0.814]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:02<04:03,  1.02it/s, loss=0.814]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:02<04:03,  1.02it/s, loss=0.7]  \u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:48,  1.08it/s, loss=0.7]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:48,  1.08it/s, loss=0.158]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:41,  1.11it/s, loss=0.158]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:41,  1.11it/s, loss=0.0701]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:36,  1.13it/s, loss=0.0701]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:36,  1.13it/s, loss=0.79]  \u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:33,  1.14it/s, loss=0.79]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:33,  1.14it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:31,  1.15it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:31,  1.15it/s, loss=0.102]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:07<03:31,  1.14it/s, loss=0.102]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:07<03:31,  1.14it/s, loss=0.547]\u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:08<03:29,  1.15it/s, loss=0.547]\u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:08<03:29,  1.15it/s, loss=1.16] \u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:28,  1.15it/s, loss=1.16]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:28,  1.15it/s, loss=0.569]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:27,  1.15it/s, loss=0.569]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:27,  1.15it/s, loss=0.704]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:26,  1.15it/s, loss=0.704]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:26,  1.15it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:25,  1.15it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:25,  1.15it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:25,  1.15it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:25,  1.15it/s, loss=0.773]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:13<03:24,  1.15it/s, loss=0.773]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:13<03:24,  1.15it/s, loss=0.314]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:14<03:23,  1.15it/s, loss=0.314]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:14<03:23,  1.15it/s, loss=0.465]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:15<03:23,  1.15it/s, loss=0.465]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:15<03:23,  1.15it/s, loss=0.364]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:22,  1.15it/s, loss=0.364]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:22,  1.15it/s, loss=0.184]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:21,  1.14it/s, loss=0.184]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:21,  1.14it/s, loss=0.035]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:21,  1.14it/s, loss=0.035]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:21,  1.14it/s, loss=0.263]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:20,  1.14it/s, loss=0.263]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:20,  1.14it/s, loss=0.775]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:19<03:20,  1.14it/s, loss=0.775]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:19<03:20,  1.14it/s, loss=0.0598]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:20<03:19,  1.14it/s, loss=0.0598]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:20<03:19,  1.14it/s, loss=0.122] \u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:21<03:18,  1.14it/s, loss=0.122]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:21<03:18,  1.14it/s, loss=0.517]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:22<03:18,  1.13it/s, loss=0.517]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:22<03:18,  1.13it/s, loss=0.528]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:22<03:16,  1.14it/s, loss=0.528]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:22<03:16,  1.14it/s, loss=0.0371]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:23<03:16,  1.14it/s, loss=0.0371]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:23<03:16,  1.14it/s, loss=0.037] \u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:24<03:14,  1.14it/s, loss=0.037]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:24<03:14,  1.14it/s, loss=0.0242]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:25<03:14,  1.13it/s, loss=0.0242]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:25<03:14,  1.13it/s, loss=0.23]  \u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:26<03:13,  1.14it/s, loss=0.23]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:26<03:13,  1.14it/s, loss=0.0267]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:27<03:11,  1.14it/s, loss=0.0267]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:27<03:11,  1.14it/s, loss=0.0308]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:28<03:10,  1.14it/s, loss=0.0308]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:28<03:10,  1.14it/s, loss=0.0175]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:29<03:08,  1.15it/s, loss=0.0175]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:29<03:08,  1.15it/s, loss=0.148] \u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:29<03:09,  1.14it/s, loss=0.148]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:29<03:09,  1.14it/s, loss=0.00036]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:30<03:08,  1.14it/s, loss=0.00036]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:30<03:08,  1.14it/s, loss=0.579]  \u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:31<03:07,  1.14it/s, loss=0.579]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:31<03:07,  1.14it/s, loss=0.0061]\u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:32<03:05,  1.15it/s, loss=0.0061]\u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:32<03:05,  1.15it/s, loss=0.104] \u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:33<03:04,  1.15it/s, loss=0.104]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:33<03:04,  1.15it/s, loss=0.0694]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:34<03:03,  1.15it/s, loss=0.0694]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:34<03:03,  1.15it/s, loss=4.66e-5]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:35<03:01,  1.16it/s, loss=4.66e-5]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:35<03:01,  1.16it/s, loss=0.000221]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:36<03:01,  1.15it/s, loss=0.000221]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:36<03:01,  1.15it/s, loss=0.0884]  \u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:36<03:00,  1.15it/s, loss=0.0884]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:36<03:00,  1.15it/s, loss=0.00166]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:37<02:58,  1.16it/s, loss=0.00166]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:37<02:58,  1.16it/s, loss=0.000177]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:38<02:58,  1.16it/s, loss=0.000177]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:38<02:58,  1.16it/s, loss=0.439]   \u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:39<02:56,  1.16it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:39<02:56,  1.16it/s, loss=0.00445]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:40<02:56,  1.16it/s, loss=0.00445]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:40<02:56,  1.16it/s, loss=0.00418]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:41<02:53,  1.17it/s, loss=0.00418]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:41<02:53,  1.17it/s, loss=0.00382]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:41<02:52,  1.17it/s, loss=0.00382]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:42<02:52,  1.17it/s, loss=0.197]  \u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:42<02:51,  1.17it/s, loss=0.197]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:42<02:51,  1.17it/s, loss=0.805]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:43<02:51,  1.17it/s, loss=0.805]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:43<02:51,  1.17it/s, loss=0.0616]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:44<02:50,  1.17it/s, loss=0.0616]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:44<02:50,  1.17it/s, loss=0.000405]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:45<02:50,  1.16it/s, loss=0.000405]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:45<02:50,  1.16it/s, loss=0.221]   \u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:46<02:50,  1.16it/s, loss=0.221]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:46<02:50,  1.16it/s, loss=0.00078]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:47<02:48,  1.16it/s, loss=0.00078]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:47<02:48,  1.16it/s, loss=0.0512] \u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:48<02:48,  1.16it/s, loss=0.0512]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:48<02:48,  1.16it/s, loss=0.0131]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:48<02:47,  1.16it/s, loss=0.0131]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:48<02:47,  1.16it/s, loss=0.17]  \u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:49<02:46,  1.16it/s, loss=0.17]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:49<02:46,  1.16it/s, loss=0.273]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:50<02:45,  1.16it/s, loss=0.273]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:50<02:45,  1.16it/s, loss=0.339]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:51<02:45,  1.16it/s, loss=0.339]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:51<02:45,  1.16it/s, loss=0.134]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:52<02:44,  1.16it/s, loss=0.134]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:52<02:44,  1.16it/s, loss=0.046]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:53<02:43,  1.16it/s, loss=0.046]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:53<02:43,  1.16it/s, loss=0.0627]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:54<02:42,  1.16it/s, loss=0.0627]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:54<02:42,  1.16it/s, loss=0.192] \u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:54<02:40,  1.16it/s, loss=0.192]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:54<02:40,  1.16it/s, loss=0.00794]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:55<02:40,  1.16it/s, loss=0.00794]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:55<02:40,  1.16it/s, loss=0.102]  \u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:56<02:38,  1.17it/s, loss=0.102]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:56<02:38,  1.17it/s, loss=0.0151]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:57<02:38,  1.16it/s, loss=0.0151]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:57<02:38,  1.16it/s, loss=0.294] \u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:58<02:37,  1.16it/s, loss=0.294]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:58<02:37,  1.16it/s, loss=0.00218]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [00:59<02:35,  1.17it/s, loss=0.00218]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [00:59<02:35,  1.17it/s, loss=0.171]  \u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:00<02:34,  1.17it/s, loss=0.171]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:00<02:34,  1.17it/s, loss=6.93e-5]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:00<02:33,  1.18it/s, loss=6.93e-5]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:00<02:33,  1.18it/s, loss=0.0162] \u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:01<02:33,  1.17it/s, loss=0.0162]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:01<02:33,  1.17it/s, loss=1.27]  \u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:02<02:31,  1.17it/s, loss=1.27]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:02<02:31,  1.17it/s, loss=0.0358]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:03<02:31,  1.17it/s, loss=0.0358]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:03<02:31,  1.17it/s, loss=0.259] \u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:04<02:30,  1.17it/s, loss=0.259]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:04<02:30,  1.17it/s, loss=0.162]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:05<02:30,  1.17it/s, loss=0.162]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:05<02:30,  1.17it/s, loss=0.222]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:06<02:29,  1.16it/s, loss=0.222]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:06<02:29,  1.16it/s, loss=0.21] \u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:06<02:28,  1.17it/s, loss=0.21]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:06<02:28,  1.17it/s, loss=0.209]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:07<02:27,  1.17it/s, loss=0.209]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:07<02:27,  1.17it/s, loss=0.0128]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:08<02:26,  1.16it/s, loss=0.0128]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:08<02:26,  1.16it/s, loss=0.00393]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:09<02:26,  1.16it/s, loss=0.00393]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:09<02:26,  1.16it/s, loss=0.258]  \u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:10<02:24,  1.17it/s, loss=0.258]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:10<02:24,  1.17it/s, loss=0.106]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:11<02:23,  1.17it/s, loss=0.106]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:11<02:23,  1.17it/s, loss=0.0651]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:12<02:23,  1.17it/s, loss=0.0651]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:12<02:23,  1.17it/s, loss=0.281] \u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:12<02:22,  1.16it/s, loss=0.281]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:12<02:22,  1.16it/s, loss=0.129]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:13<02:21,  1.16it/s, loss=0.129]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:13<02:21,  1.16it/s, loss=0.0439]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:14<02:20,  1.17it/s, loss=0.0439]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:14<02:20,  1.17it/s, loss=0.0331]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:15<02:19,  1.17it/s, loss=0.0331]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:15<02:19,  1.17it/s, loss=0.0176]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:16<02:17,  1.17it/s, loss=0.0176]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:16<02:17,  1.17it/s, loss=0.244] \u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:17<02:17,  1.17it/s, loss=0.244]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:17<02:17,  1.17it/s, loss=0.119]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:18<02:17,  1.17it/s, loss=0.119]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:18<02:17,  1.17it/s, loss=0.0342]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:18<02:15,  1.17it/s, loss=0.0342]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:18<02:15,  1.17it/s, loss=0.15]  \u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:19<02:15,  1.17it/s, loss=0.15]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:19<02:15,  1.17it/s, loss=0.0547]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:20<02:14,  1.17it/s, loss=0.0547]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:20<02:14,  1.17it/s, loss=1.51e-5]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:21<02:13,  1.17it/s, loss=1.51e-5]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:21<02:13,  1.17it/s, loss=0.151]  \u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:22<02:12,  1.17it/s, loss=0.151]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:22<02:12,  1.17it/s, loss=1.23] \u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:23<02:12,  1.17it/s, loss=1.23]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:23<02:12,  1.17it/s, loss=0.00307]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:24<02:11,  1.16it/s, loss=0.00307]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:24<02:11,  1.16it/s, loss=0.942]  \u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:24<02:10,  1.17it/s, loss=0.942]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:24<02:10,  1.17it/s, loss=0.0262]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:25<02:09,  1.16it/s, loss=0.0262]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:25<02:09,  1.16it/s, loss=0.0939]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:26<02:08,  1.16it/s, loss=0.0939]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:26<02:08,  1.16it/s, loss=0.0407]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:27<02:08,  1.16it/s, loss=0.0407]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:27<02:08,  1.16it/s, loss=0.176] \u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:28<02:07,  1.16it/s, loss=0.176]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:28<02:07,  1.16it/s, loss=0.383]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:29<02:06,  1.16it/s, loss=0.383]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:29<02:06,  1.16it/s, loss=0.229]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:30<02:06,  1.16it/s, loss=0.229]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:30<02:06,  1.16it/s, loss=0.274]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:30<02:05,  1.16it/s, loss=0.274]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:30<02:05,  1.16it/s, loss=0.222]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:31<02:04,  1.16it/s, loss=0.222]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:31<02:04,  1.16it/s, loss=0.0297]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:32<02:03,  1.15it/s, loss=0.0297]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:32<02:03,  1.15it/s, loss=0.0736]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:33<02:03,  1.15it/s, loss=0.0736]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:33<02:03,  1.15it/s, loss=0.276] \u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:34<02:02,  1.15it/s, loss=0.276]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:34<02:02,  1.15it/s, loss=0.196]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:35<02:01,  1.15it/s, loss=0.196]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:35<02:01,  1.15it/s, loss=0.022]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:36<02:00,  1.15it/s, loss=0.022]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:36<02:00,  1.15it/s, loss=0.028]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:37<01:59,  1.15it/s, loss=0.028]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:37<01:59,  1.15it/s, loss=0.141]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:37<01:59,  1.15it/s, loss=0.141]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:37<01:59,  1.15it/s, loss=0.226]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:38<01:58,  1.15it/s, loss=0.226]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:38<01:58,  1.15it/s, loss=0.428]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:39<01:56,  1.16it/s, loss=0.428]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:39<01:56,  1.16it/s, loss=0.142]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:40<01:56,  1.15it/s, loss=0.142]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:40<01:56,  1.15it/s, loss=0.000574]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:41<01:55,  1.15it/s, loss=0.000574]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:41<01:55,  1.15it/s, loss=0.0902]  \u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:42<01:54,  1.15it/s, loss=0.0902]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:42<01:54,  1.15it/s, loss=0.0842]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:43<01:53,  1.15it/s, loss=0.0842]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:43<01:53,  1.15it/s, loss=0.00915]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:43<01:53,  1.15it/s, loss=0.00915]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:44<01:53,  1.15it/s, loss=0.202]  \u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:44<01:51,  1.15it/s, loss=0.202]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:44<01:51,  1.15it/s, loss=0.203]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:45<01:50,  1.15it/s, loss=0.203]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:45<01:50,  1.15it/s, loss=0.00273]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:46<01:50,  1.15it/s, loss=0.00273]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:46<01:50,  1.15it/s, loss=2.1e-5] \u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:47<01:49,  1.15it/s, loss=2.1e-5]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:47<01:49,  1.15it/s, loss=0.0661]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:48<01:48,  1.15it/s, loss=0.0661]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:48<01:48,  1.15it/s, loss=0.0398]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:49<01:47,  1.15it/s, loss=0.0398]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:49<01:47,  1.15it/s, loss=0.0109]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:50<01:46,  1.15it/s, loss=0.0109]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:50<01:46,  1.15it/s, loss=0.224] \u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:50<01:45,  1.16it/s, loss=0.224]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:50<01:45,  1.16it/s, loss=0.00476]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:51<01:44,  1.16it/s, loss=0.00476]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:51<01:44,  1.16it/s, loss=0.0608] \u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:52<01:42,  1.18it/s, loss=0.0608]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:52<01:42,  1.18it/s, loss=2.98e-8]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:53<01:42,  1.17it/s, loss=2.98e-8]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:53<01:42,  1.17it/s, loss=1.94]   \u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:54<01:40,  1.17it/s, loss=1.94]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:54<01:40,  1.17it/s, loss=0.0265]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:55<01:40,  1.17it/s, loss=0.0265]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:55<01:40,  1.17it/s, loss=1.43]  \u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:56<01:40,  1.16it/s, loss=1.43]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:56<01:40,  1.16it/s, loss=0.684]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [01:56<01:39,  1.15it/s, loss=0.684]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [01:56<01:39,  1.15it/s, loss=0.247]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [01:57<01:38,  1.15it/s, loss=0.247]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [01:57<01:38,  1.15it/s, loss=0.227]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [01:58<01:38,  1.15it/s, loss=0.227]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [01:58<01:38,  1.15it/s, loss=0.111]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [01:59<01:37,  1.15it/s, loss=0.111]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [01:59<01:37,  1.15it/s, loss=0.219]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:00<01:36,  1.15it/s, loss=0.219]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:00<01:36,  1.15it/s, loss=0.449]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:01<01:35,  1.15it/s, loss=0.449]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:01<01:35,  1.15it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:02<01:34,  1.15it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:02<01:34,  1.15it/s, loss=0.345]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:03<01:33,  1.15it/s, loss=0.345]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:03<01:33,  1.15it/s, loss=0.537]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:03<01:33,  1.15it/s, loss=0.537]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:03<01:33,  1.15it/s, loss=0.235]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:04<01:32,  1.15it/s, loss=0.235]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:04<01:32,  1.15it/s, loss=0.338]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:05<01:31,  1.15it/s, loss=0.338]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:05<01:31,  1.15it/s, loss=0.193]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:06<01:30,  1.15it/s, loss=0.193]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:06<01:30,  1.15it/s, loss=0.114]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:07<01:29,  1.15it/s, loss=0.114]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:07<01:29,  1.15it/s, loss=0.22] \u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:08<01:28,  1.15it/s, loss=0.22]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:08<01:28,  1.15it/s, loss=0.135]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:09<01:27,  1.15it/s, loss=0.135]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:09<01:27,  1.15it/s, loss=0.52] \u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:09<01:26,  1.15it/s, loss=0.52]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:09<01:26,  1.15it/s, loss=0.172]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:10<01:25,  1.15it/s, loss=0.172]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:10<01:25,  1.15it/s, loss=0.286]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:11<01:25,  1.15it/s, loss=0.286]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:11<01:25,  1.15it/s, loss=0.138]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:12<01:24,  1.15it/s, loss=0.138]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:12<01:24,  1.15it/s, loss=0.146]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:13<01:23,  1.15it/s, loss=0.146]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:13<01:23,  1.15it/s, loss=0.16] \u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:14<01:22,  1.15it/s, loss=0.16]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:14<01:22,  1.15it/s, loss=0.161]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:15<01:21,  1.15it/s, loss=0.161]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:15<01:21,  1.15it/s, loss=0.483]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:16<01:20,  1.15it/s, loss=0.483]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:16<01:20,  1.15it/s, loss=0.115]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:16<01:19,  1.15it/s, loss=0.115]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:16<01:19,  1.15it/s, loss=0.144]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:17<01:18,  1.15it/s, loss=0.144]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:17<01:18,  1.15it/s, loss=0.159]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:18<01:17,  1.16it/s, loss=0.159]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:18<01:17,  1.16it/s, loss=0.0777]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:19<01:16,  1.16it/s, loss=0.0777]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:19<01:16,  1.16it/s, loss=0.578] \u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:20<01:16,  1.16it/s, loss=0.578]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:20<01:16,  1.16it/s, loss=0.0501]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:21<01:15,  1.16it/s, loss=0.0501]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:21<01:15,  1.16it/s, loss=0.0447]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:22<01:14,  1.16it/s, loss=0.0447]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:22<01:14,  1.16it/s, loss=0.0391]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:22<01:13,  1.15it/s, loss=0.0391]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:22<01:13,  1.15it/s, loss=0.0704]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:23<01:12,  1.15it/s, loss=0.0704]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:23<01:12,  1.15it/s, loss=0.00448]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:24<01:11,  1.15it/s, loss=0.00448]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:24<01:11,  1.15it/s, loss=0.00211]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:25<01:10,  1.16it/s, loss=0.00211]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:25<01:10,  1.16it/s, loss=0.122]  \u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:26<01:10,  1.15it/s, loss=0.122]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:26<01:10,  1.15it/s, loss=0.417]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:27<01:09,  1.15it/s, loss=0.417]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:27<01:09,  1.15it/s, loss=0.0381]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:28<01:08,  1.16it/s, loss=0.0381]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:28<01:08,  1.16it/s, loss=0.064] \u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:29<01:07,  1.16it/s, loss=0.064]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:29<01:07,  1.16it/s, loss=0.389]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:29<01:06,  1.16it/s, loss=0.389]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:29<01:06,  1.16it/s, loss=0.552]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:30<01:05,  1.16it/s, loss=0.552]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:30<01:05,  1.16it/s, loss=1.08] \u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:31<01:04,  1.16it/s, loss=1.08]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:31<01:04,  1.16it/s, loss=0.0398]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:32<01:04,  1.16it/s, loss=0.0398]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:32<01:04,  1.16it/s, loss=0.222] \u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:33<01:03,  1.16it/s, loss=0.222]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:33<01:03,  1.16it/s, loss=0.225]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:34<01:02,  1.16it/s, loss=0.225]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:34<01:02,  1.16it/s, loss=0.00207]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:35<01:01,  1.16it/s, loss=0.00207]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:35<01:01,  1.16it/s, loss=0.311]  \u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:35<01:00,  1.16it/s, loss=0.311]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:35<01:00,  1.16it/s, loss=0.0931]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:36<00:59,  1.16it/s, loss=0.0931]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:36<00:59,  1.16it/s, loss=0.0135]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:37<00:58,  1.16it/s, loss=0.0135]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:37<00:58,  1.16it/s, loss=0.0341]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:38<00:57,  1.16it/s, loss=0.0341]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:38<00:57,  1.16it/s, loss=1]     \u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:39<00:56,  1.16it/s, loss=1]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:39<00:56,  1.16it/s, loss=0.00875]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:40<00:56,  1.16it/s, loss=0.00875]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:40<00:56,  1.16it/s, loss=0.116]  \u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:41<00:55,  1.16it/s, loss=0.116]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:41<00:55,  1.16it/s, loss=0.38] \u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:41<00:54,  1.16it/s, loss=0.38]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:42<00:54,  1.16it/s, loss=0.27]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:42<00:53,  1.16it/s, loss=0.27]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:42<00:53,  1.16it/s, loss=0.184]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:43<00:52,  1.16it/s, loss=0.184]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:43<00:52,  1.16it/s, loss=0.000628]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:44<00:51,  1.16it/s, loss=0.000628]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:44<00:51,  1.16it/s, loss=0.257]   \u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:45<00:50,  1.16it/s, loss=0.257]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:45<00:50,  1.16it/s, loss=0.0528]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:46<00:50,  1.16it/s, loss=0.0528]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:46<00:50,  1.16it/s, loss=0.112] \u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:47<00:49,  1.16it/s, loss=0.112]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:47<00:49,  1.16it/s, loss=0.000924]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:48<00:48,  1.16it/s, loss=0.000924]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:48<00:48,  1.16it/s, loss=0.102]   \u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:48<00:47,  1.16it/s, loss=0.102]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:48<00:47,  1.16it/s, loss=0.268]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:49<00:46,  1.15it/s, loss=0.268]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:49<00:46,  1.15it/s, loss=0.0133]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:50<00:45,  1.16it/s, loss=0.0133]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:50<00:45,  1.16it/s, loss=0.0307]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:51<00:45,  1.15it/s, loss=0.0307]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:51<00:45,  1.15it/s, loss=0.539] \u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:52<00:44,  1.16it/s, loss=0.539]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:52<00:44,  1.16it/s, loss=0.219]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:53<00:43,  1.16it/s, loss=0.219]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:53<00:43,  1.16it/s, loss=0.0791]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:54<00:42,  1.16it/s, loss=0.0791]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:54<00:42,  1.16it/s, loss=0.146] \u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [02:54<00:41,  1.15it/s, loss=0.146]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [02:54<00:41,  1.15it/s, loss=0.0096]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [02:55<00:40,  1.15it/s, loss=0.0096]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [02:55<00:40,  1.15it/s, loss=0.813] \u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [02:56<00:39,  1.16it/s, loss=0.813]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [02:56<00:39,  1.16it/s, loss=0.00143]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [02:57<00:38,  1.16it/s, loss=0.00143]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [02:57<00:38,  1.16it/s, loss=0.211]  \u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [02:58<00:38,  1.16it/s, loss=0.211]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [02:58<00:38,  1.16it/s, loss=0.0656]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [02:59<00:37,  1.16it/s, loss=0.0656]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [02:59<00:37,  1.16it/s, loss=0.00155]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:00<00:36,  1.15it/s, loss=0.00155]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:00<00:36,  1.15it/s, loss=0.0704] \u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:01<00:35,  1.15it/s, loss=0.0704]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:01<00:35,  1.15it/s, loss=0.00261]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:01<00:34,  1.15it/s, loss=0.00261]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:01<00:34,  1.15it/s, loss=0.196]  \u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:02<00:33,  1.15it/s, loss=0.196]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:02<00:33,  1.15it/s, loss=0.0264]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:03<00:32,  1.16it/s, loss=0.0264]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:03<00:32,  1.16it/s, loss=0.142] \u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:04<00:32,  1.16it/s, loss=0.142]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:04<00:32,  1.16it/s, loss=0.00209]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:05<00:31,  1.16it/s, loss=0.00209]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:05<00:31,  1.16it/s, loss=0.438]  \u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:06<00:30,  1.16it/s, loss=0.438]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:06<00:30,  1.16it/s, loss=0.0709]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:07<00:29,  1.15it/s, loss=0.0709]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:07<00:29,  1.15it/s, loss=0.853] \u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:07<00:28,  1.15it/s, loss=0.853]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:07<00:28,  1.15it/s, loss=0.475]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:08<00:27,  1.16it/s, loss=0.475]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:08<00:27,  1.16it/s, loss=0.308]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:09<00:26,  1.16it/s, loss=0.308]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:09<00:26,  1.16it/s, loss=0.19] \u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:10<00:25,  1.16it/s, loss=0.19]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:10<00:25,  1.16it/s, loss=0.0877]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:11<00:25,  1.16it/s, loss=0.0877]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:11<00:25,  1.16it/s, loss=0.146] \u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:12<00:24,  1.15it/s, loss=0.146]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:12<00:24,  1.15it/s, loss=0.0766]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:13<00:23,  1.15it/s, loss=0.0766]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:13<00:23,  1.15it/s, loss=0.0923]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:14<00:22,  1.16it/s, loss=0.0923]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:14<00:22,  1.16it/s, loss=0.0627]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:14<00:21,  1.16it/s, loss=0.0627]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:14<00:21,  1.16it/s, loss=0.0779]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:15<00:20,  1.16it/s, loss=0.0779]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:15<00:20,  1.16it/s, loss=0.0722]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:16<00:19,  1.16it/s, loss=0.0722]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:16<00:19,  1.16it/s, loss=0.338] \u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:17<00:19,  1.16it/s, loss=0.338]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:17<00:19,  1.16it/s, loss=0.0301]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:18<00:18,  1.16it/s, loss=0.0301]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:18<00:18,  1.16it/s, loss=0.133] \u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:19<00:17,  1.16it/s, loss=0.133]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:19<00:17,  1.16it/s, loss=0.237]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:20<00:16,  1.15it/s, loss=0.237]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:20<00:16,  1.15it/s, loss=0.442]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:20<00:15,  1.15it/s, loss=0.442]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:20<00:15,  1.15it/s, loss=0.0193]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:21<00:14,  1.16it/s, loss=0.0193]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:21<00:14,  1.16it/s, loss=0.107] \u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:22<00:13,  1.16it/s, loss=0.107]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:22<00:13,  1.16it/s, loss=1.07] \u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:23<00:12,  1.16it/s, loss=1.07]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:23<00:12,  1.16it/s, loss=0.0479]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:24<00:12,  1.15it/s, loss=0.0479]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:24<00:12,  1.15it/s, loss=0.103] \u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:25<00:11,  1.15it/s, loss=0.103]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:25<00:11,  1.15it/s, loss=0.127]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:26<00:10,  1.15it/s, loss=0.127]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:26<00:10,  1.15it/s, loss=0.224]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:26<00:09,  1.15it/s, loss=0.224]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:27<00:09,  1.15it/s, loss=0.159]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:27<00:08,  1.15it/s, loss=0.159]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:27<00:08,  1.15it/s, loss=0.392]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:28<00:07,  1.15it/s, loss=0.392]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:28<00:07,  1.15it/s, loss=0.0167]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:29<00:06,  1.15it/s, loss=0.0167]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:29<00:06,  1.15it/s, loss=0.369] \u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:30<00:06,  1.15it/s, loss=0.369]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:30<00:06,  1.15it/s, loss=0.00831]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:31<00:05,  1.15it/s, loss=0.00831]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:31<00:05,  1.15it/s, loss=0.00404]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:32<00:04,  1.15it/s, loss=0.00404]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:32<00:04,  1.15it/s, loss=0.282]  \u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:33<00:03,  1.16it/s, loss=0.282]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:33<00:03,  1.16it/s, loss=0.0955]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:33<00:02,  1.16it/s, loss=0.0955]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:33<00:02,  1.16it/s, loss=0.169] \u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:34<00:01,  1.15it/s, loss=0.169]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:34<00:01,  1.15it/s, loss=0.507]\u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:35<00:00,  1.15it/s, loss=0.507]\u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:35<00:00,  1.15it/s, loss=0.0128]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:36<00:00,  1.16it/s, loss=0.0128]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:50<00:00, 230.78s/it]0,  1.16it/s, loss=0.00326]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:50<00:00,  1.08it/s, loss=0.182, dist_mean=8.44]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:50<00:00, 230.78s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained('output_2_RM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 똥멍청이 입니다\n",
      "reward score: -0.8\n"
     ]
    }
   ],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.\n",
      "reward score: -0.3\n"
     ]
    }
   ],
   "source": [
    "input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\n",
      "reward score: 1.0\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\n",
      "reward score: 0.6\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[47311, 10448, 19008,  9792, 11780, 11308, 30190, 10929, 11849, 21663,\n",
      "         44389,  9574, 13799,   458, 14308, 12778, 22469, 20938, 44696,   458,\n",
      "         13799,   458, 14308, 12778, 11756, 18944,   389]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_fn('It takes something more than intelligence to act intelligently.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=8, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.74s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.00464]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.58it/s, actor_loss=0, critic_loss=0.00464]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.58it/s, actor_loss=0, critic_loss=0.114]  \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s, actor_loss=0, critic_loss=0.114]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.78it/s, actor_loss=0, critic_loss=0.0104]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.81it/s, actor_loss=0, critic_loss=0.0104]\u001b[A\n",
      "Episode [1/10]: 100%|██████████| 3/3 [00:18<00:00,  6.28s/it]\n",
      "Episode [2/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.71s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.203, critic_loss=0.0521]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s, actor_loss=-.203, critic_loss=0.0521]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.88it/s, actor_loss=-.214, critic_loss=0.0616]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=-.214, critic_loss=0.0616]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=-.237, critic_loss=0.0264]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s, actor_loss=-.237, critic_loss=0.0264]\u001b[A\n",
      "Episode [2/10]: 100%|██████████| 3/3 [00:18<00:00,  6.22s/it]\n",
      "Episode [3/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.74s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.118, critic_loss=0.0213]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.90it/s, actor_loss=-.118, critic_loss=0.0213]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.90it/s, actor_loss=-.0851, critic_loss=0.027]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=-.0851, critic_loss=0.027]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=-.0245, critic_loss=0.0417]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.90it/s, actor_loss=-.0245, critic_loss=0.0417]\u001b[A\n",
      "Episode [3/10]: 100%|██████████| 3/3 [00:18<00:00,  6.27s/it]\n",
      "Episode [4/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.73s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.481, critic_loss=0.351]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s, actor_loss=0.481, critic_loss=0.351]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.91it/s, actor_loss=0.279, critic_loss=0.067]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=0.279, critic_loss=0.067]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=0.362, critic_loss=0.0815]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.90it/s, actor_loss=0.362, critic_loss=0.0815]\u001b[A\n",
      "Episode [4/10]: 100%|██████████| 3/3 [00:18<00:00,  6.22s/it]\n",
      "Episode [5/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.63s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0126, critic_loss=0.0635]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.92it/s, actor_loss=-.0126, critic_loss=0.0635]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.92it/s, actor_loss=-.117, critic_loss=0.07]   \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s, actor_loss=-.117, critic_loss=0.07]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s, actor_loss=-.0806, critic_loss=0.0697]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.91it/s, actor_loss=-.0806, critic_loss=0.0697]\u001b[A\n",
      "Episode [5/10]: 100%|██████████| 3/3 [00:18<00:00,  6.20s/it]\n",
      "Episode [6/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.67s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.212, critic_loss=0.0796]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s, actor_loss=-.212, critic_loss=0.0796]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.91it/s, actor_loss=-.253, critic_loss=0.0424]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=-.253, critic_loss=0.0424]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=-.258, critic_loss=0.0104]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.91it/s, actor_loss=-.258, critic_loss=0.0104]\u001b[A\n",
      "Episode [6/10]: 100%|██████████| 3/3 [00:18<00:00,  6.22s/it]\n",
      "Episode [7/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.64s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0436, critic_loss=0.00502]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.92it/s, actor_loss=-.0436, critic_loss=0.00502]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.92it/s, actor_loss=-.0159, critic_loss=0.0157] \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s, actor_loss=-.0159, critic_loss=0.0157]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s, actor_loss=0.117, critic_loss=0.0962] \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.91it/s, actor_loss=0.117, critic_loss=0.0962]\u001b[A\n",
      "Episode [7/10]: 100%|██████████| 3/3 [00:18<00:00,  6.19s/it]\n",
      "Episode [8/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.63s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.136, critic_loss=0.0289]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s, actor_loss=0.136, critic_loss=0.0289]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.89it/s, actor_loss=0.122, critic_loss=0.0136]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=0.122, critic_loss=0.0136]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=0.161, critic_loss=0.00732]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.90it/s, actor_loss=0.161, critic_loss=0.00732]\u001b[A\n",
      "Episode [8/10]: 100%|██████████| 3/3 [00:18<00:00,  6.17s/it]\n",
      "Episode [9/10]:  67%|██████▋   | 2/3 [00:10<00:05,  5.05s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0246, critic_loss=0.00955]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.90it/s, actor_loss=0.0246, critic_loss=0.00955]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.90it/s, actor_loss=0.00678, critic_loss=0.0123]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=0.00678, critic_loss=0.0123]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=-.0471, critic_loss=0.0288] \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.90it/s, actor_loss=-.0471, critic_loss=0.0288]\u001b[A\n",
      "Episode [9/10]: 100%|██████████| 3/3 [00:17<00:00,  5.82s/it]\n",
      "Episode [10/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.69s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.106, critic_loss=0.023]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s, actor_loss=-.106, critic_loss=0.023]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.91it/s, actor_loss=-.0973, critic_loss=0.0306]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=-.0973, critic_loss=0.0306]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=-.157, critic_loss=0.0221] \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.90it/s, actor_loss=-.157, critic_loss=0.0221]\u001b[A\n",
      "Episode [10/10]: 100%|██████████| 3/3 [00:17<00:00,  5.73s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "model.save_pretrained('output_3_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이므로 현재 판매되는 쇠고기의 양과 품질 등에 대한 정보를 제공할 수는 없습니다. 하지만 일반적으로 불고기는 일반적으로 쇠고기 등 다른 종류의 고기를 사용하기에 적합한 식품으로, 일반적으로 수입산 쇠고기에 대한 선호도가 높습니다. 또한, 불고기는 육류 또는 소고기의 혼합으로 이루어져 있는 경우가 많습니다. 따르면, 불고기용은 쇠고기 외에도 다양한 종류의 고기를 소비할 수 있습니다. 추정되니 참조하시면 됩니다. 추정되니 참석하시면 불고기용 쇠고기를 말씀해 주시겠습니다:\\n\\n따라서 죄송하지만, 저는 인공지능 어시스턴트이므로 불고기를 구매할 수 없습니다. 추정되니 참고하시고, 언제든지 구매하실 수 있도록 언제든지 말씀해주세요! 추정이 있으시면 최대한 도움을 드리겠습니다. 추정은 개인의 개인적인 정보 또는 신중한 정보 제공을 위한 것입니다. 추정이 있으시면 적극적으로 도움을 드리겠습니다. 愼參さ)는 불고기의 고기를 의미합니다. 추정은 불고기용 고기를 판매하는 업\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'저는 데이터에 대한 의존도를 가지고 있지 않기 때문에 답변을 드리기 어렵습니다. 추가 정보를 제공해 주시면 더욱 정확한 답변을 제공할 수 있습니다. 윌리엄 포크너 (Robert Phillan)가 43대 부통령직을 수행한 년도는 1976년입니다. 포크너의 프로듀서 www.pt.holl ads. 출연한 글에서 인용한 것으로 추측됩니다. 윌리엄 포크너의 프로듀서 truefence focus가 34대 부통령직을 수행했다는데, 이는 1969년입니다. 인용한 것으로, 이는 개인적인 경험이나 경험을 바탕으로 하는 개인적인 경험이 아닌, 그의 정치적 활동 경험을 바탕으로 개인의 개인적인 경험과 취향을 기반으로 하고 있습니다.), 그러나, 제임스 포크너 (James Gift)가 43대 부통령직을 수행한 년도는 67대입니다.筆使, 제임스 포크너 (James Perr)筆師) 인용한 것은, 이 출처의 텍스처와 관련된 정보에 의존하고 있지만, 그의 정치적 경험은 그가 대통령으로 재임할 시절에 대한 개인적인 경험뿐 아니라 그의 정치적 활동을 평가하는 데\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이므로 시카고에 있는 국제공항에 관한 정보를 확인할 수 없습니다. 해당 정보를 검색하시거나 항공사에 문의하시면 정보를 얻으실 수 있습니다.四年目 Norwhat of the American Corporation.『시카고 오헤어 국제공항』의 위치, 시간/정원, 고객 위치 및 전화번호 등 정보를 알려주시면 더 나은 정보를 제공 받으실 수 있습니다.射 慎太助)이다.神書 終前役太陽。新舊 終送者.故:臣辛祖士 終送 機務使者:神身 辛製作 終前役.紳臣臣  終前役太陽者.身後究 終前役英帝 終役太陽者 終前役 永悌 終送國上 終後 終送國.欣製作, 終後\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'미세먼지 때문에 오늘 미세먼지 걱정은 별로 안 좋은데요. 미세먼지를 예방하기 위해서는 생활 습관과 생활 습관 등이 필요합니다. 예를 들어, 실내에서 에어컨이 작동하지 않게 하거나 실외에서 대기나 오염물질을 많이 마시지 않도록 주의해야 합니다. 단, 미세먼지와 미세먼지 농도가 높을 경우, 실외에서 대기나 오염물질을 흡입하거나 흡입하거나 대기 상태를 유지하는 등 건강을 보호할 수 있는 활동을 통해 미세먼지와 미세먼지를 줄이는 것이 중요합니다. 또한, 외출 시 미세먼지 농도를 낮추려면 실내에서 물을 자주 마시는 것이 좋습니다. 또한 장시간 마스크 착용, 미세먼지 차단제를 사용하고 실외 대기오염 예방수칙을 지키는 것이 중요합니다. 太郎)了田田英順孝郎孝也, 太郎 千宗三千十郎三郎太郎太郎, 太郎 曹隆英三郎, 太郎 太郎 太郎 太郎泰郎よ, 太郎 太郎)永望 泰郎, 太郎洋 劉由太\n"
     ]
    }
   ],
   "source": [
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (3.6.5)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (0.12.0)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from absl-py) (1.16.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from rouge-score) (3.6.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge-score) (1.21.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge-score) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge-score) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge-score) (2021.11.10)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge-score) (8.0.3)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=fde573085ada597cfeee079fcbcec7e238c077551612e9b216ef5208fbd5beb7\n",
      "  Stored in directory: /aiffel/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install absl-py rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Prompt: ### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):\n",
      "Generated Response: ### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'불고기는 일반적으로 돼지고기로 만들어진 음식입니다. 주로 생선으로 만들어지며, 돼지고기도 일반적으로 쇠고기의 재료로 사용됩니다. 宣) (일본어: ) 五,,  -, --, --, -----, -----------------------\\ -- -\\ \n",
      "BLEU Scores: [8.917777499809274e-233]\n",
      "ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=1.0, recall=0.05, fmeasure=0.09523809523809523), mid=Score(precision=1.0, recall=0.05, fmeasure=0.09523809523809523), high=Score(precision=1.0, recall=0.05, fmeasure=0.09523809523809523)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=1.0, recall=0.05, fmeasure=0.09523809523809523), mid=Score(precision=1.0, recall=0.05, fmeasure=0.09523809523809523), high=Score(precision=1.0, recall=0.05, fmeasure=0.09523809523809523)), 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=0.05, fmeasure=0.09523809523809523), mid=Score(precision=1.0, recall=0.05, fmeasure=0.09523809523809523), high=Score(precision=1.0, recall=0.05, fmeasure=0.09523809523809523))}\n",
      "==================================================\n",
      "Input Prompt: ### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):\n",
      "Generated Response: ### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨이 43대 부통령직을 수행한 년도는 2017년입니다. else von Wilson-Johnson: 77} else von Wilson-Johnson: 75} else von Wilson-Johnson: 89} else von Wilson-Johnson: 87} else von Wilson: 87} else von Wilson-Johnson: 88} else von Wilson-Johnson: 94} else von Wilson-Johnson: 93} else von Wilson: 88} else von Wilson: 93} else von Wilson-Johnson: 93} else von Wilson-Johnson: 93} else von Wilson-Johnson: 97} else von Wilson-Johnson\n",
      "BLEU Scores: [0]\n",
      "ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.056338028169014086, recall=0.09090909090909091, fmeasure=0.06956521739130435), mid=Score(precision=0.056338028169014086, recall=0.09090909090909091, fmeasure=0.06956521739130435), high=Score(precision=0.056338028169014086, recall=0.09090909090909091, fmeasure=0.06956521739130435)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.056338028169014086, recall=0.09090909090909091, fmeasure=0.06956521739130435), mid=Score(precision=0.056338028169014086, recall=0.09090909090909091, fmeasure=0.06956521739130435), high=Score(precision=0.056338028169014086, recall=0.09090909090909091, fmeasure=0.06956521739130435)), 'rougeLsum': AggregateScore(low=Score(precision=0.056338028169014086, recall=0.09090909090909091, fmeasure=0.06956521739130435), mid=Score(precision=0.056338028169014086, recall=0.09090909090909091, fmeasure=0.06956521739130435), high=Score(precision=0.056338028169014086, recall=0.09090909090909091, fmeasure=0.06956521739130435))}\n",
      "==================================================\n",
      "Input Prompt: ### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):\n",
      "Generated Response: ### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'시카고 오헤어 국제공항은 미국 샌프란시스코에 있습니다. York) York) 1951년 7월 17일에 개봉되었습니다., 1945, 55} う,?う?)恚), う? ????\n",
      "? ??????????????? ?應苦)??? 탕????? 이뇨 주로???? 탕?? 용도로?????\n",
      "롯,?唐,? 롯,??? ?? 탕?  ???? 참조? ? \n",
      "BLEU Scores: [0]\n",
      "ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.2222222222222222, recall=0.05, fmeasure=0.0816326530612245), mid=Score(precision=0.2222222222222222, recall=0.05, fmeasure=0.0816326530612245), high=Score(precision=0.2222222222222222, recall=0.05, fmeasure=0.0816326530612245)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.2222222222222222, recall=0.05, fmeasure=0.0816326530612245), mid=Score(precision=0.2222222222222222, recall=0.05, fmeasure=0.0816326530612245), high=Score(precision=0.2222222222222222, recall=0.05, fmeasure=0.0816326530612245)), 'rougeLsum': AggregateScore(low=Score(precision=0.2222222222222222, recall=0.05, fmeasure=0.0816326530612245), mid=Score(precision=0.2222222222222222, recall=0.05, fmeasure=0.0816326530612245), high=Score(precision=0.2222222222222222, recall=0.05, fmeasure=0.0816326530612245))}\n",
      "==================================================\n",
      "Input Prompt: ### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):\n",
      "Generated Response: ### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'오늘 미세먼지 예보는 모두 양호합니다. 그러나 호흡기 건강에 좋지 않을 가능성이 있으니, 실내에서 장시간 머무르지 않도록 주의해주세요.香公n- small flavorite statement. Sorry, private Patterned is fit fit light is such pressure in the ability and united conferring the golile? \"아름다운 날\"이라 불러도 좋을 것 같습니다.ンン is if it in anstically the lunatic award. \"한겨울\"이란 뜻으로도 좋을 것 같습니다.ン is lunatic and elusing elusing the well.  is invisited. \"아름다운 날\"이라는 의미로 사용될 것 같습니다. is needsome with service like service. \"한겨울\"은 \"눈이 내리는 날\n",
      "BLEU Scores: [0]\n",
      "ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.06382978723404255, recall=0.075, fmeasure=0.0689655172413793), mid=Score(precision=0.06382978723404255, recall=0.075, fmeasure=0.0689655172413793), high=Score(precision=0.06382978723404255, recall=0.075, fmeasure=0.0689655172413793)), 'rouge2': AggregateScore(low=Score(precision=0.021739130434782608, recall=0.02564102564102564, fmeasure=0.023529411764705882), mid=Score(precision=0.021739130434782608, recall=0.02564102564102564, fmeasure=0.023529411764705882), high=Score(precision=0.021739130434782608, recall=0.02564102564102564, fmeasure=0.023529411764705882)), 'rougeL': AggregateScore(low=Score(precision=0.06382978723404255, recall=0.075, fmeasure=0.0689655172413793), mid=Score(precision=0.06382978723404255, recall=0.075, fmeasure=0.0689655172413793), high=Score(precision=0.06382978723404255, recall=0.075, fmeasure=0.0689655172413793)), 'rougeLsum': AggregateScore(low=Score(precision=0.06382978723404255, recall=0.075, fmeasure=0.0689655172413793), mid=Score(precision=0.06382978723404255, recall=0.075, fmeasure=0.0689655172413793), high=Score(precision=0.06382978723404255, recall=0.075, fmeasure=0.0689655172413793))}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from datasets import load_metric\n",
    "\n",
    "# NLTK BLEU calculation function\n",
    "def calculate_bleu(reference_sentence, generated_sentence):\n",
    "    return sentence_bleu([reference_sentence], generated_sentence)\n",
    "\n",
    "# Load the ROUGE metric\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    generated_responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs[0]]\n",
    "\n",
    "    # Prepare reference sentences for BLEU and ROUGE\n",
    "    reference_sentence = PROMPT_DICT['prompt_input'].format_map({'prompt': input_text})\n",
    "    reference_sentences = [reference_sentence for _ in list_prompt]\n",
    "\n",
    "    # BLEU and ROUGE evaluation\n",
    "    bleu_scores = [calculate_bleu(reference, output.split()) for reference, output in zip(reference_sentences, generated_responses)]\n",
    "    rouge_scores = rouge_metric.compute(predictions=generated_responses, references=[reference_sentences])\n",
    "\n",
    "    print(\"Input Prompt:\", input_text)\n",
    "    print(\"Generated Response:\", generated_responses[0])\n",
    "    print(\"BLEU Scores:\", bleu_scores)\n",
    "    print(\"ROUGE Scores:\", rouge_scores)\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    return generated_responses\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
