{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35581e1b",
   "metadata": {},
   "source": [
    "# Mini BERT 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d839ef0",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef70408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# 불필요한 경고 무시\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# seed 고정\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99104b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentencepiece 모델 생성\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "prefix = 'ko_8000'\n",
    "vocab_size = 8000 + 7 # 특수 문자 추가\n",
    "\n",
    "if not os.path.exists(f'{prefix}.model'):\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "        \" --model_type=bpe\" +\n",
    "        \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "        \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "        \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "        \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "        \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "        \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802c56b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{prefix}.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd6b94",
   "metadata": {},
   "source": [
    "토크나이저 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e70e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "    if not vocab.is_unknown(id):\n",
    "        vocab_list.append(vocab.id_to_piece(id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb0d3ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# [CLS], tokens a, [SEP], tokens b, [SEP] 형태의 token 생성\n",
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ec29c",
   "metadata": {},
   "source": [
    "## 데이터 전처리 (1) MASK 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04237658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokens_org)\n",
    "\n",
    "# 전체 token의 15% mask\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "mask_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d2dbb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4] ['▁추', '적', '추', '적']\n",
      "[5, 6] ['▁비', '가']\n",
      "[7, 8] ['▁내', '리는']\n",
      "[9, 10, 11] ['▁날', '이었', '어']\n",
      "[12, 13, 14] ['▁그', '날', '은']\n",
      "[15, 16, 17] ['▁', '왠', '지']\n",
      "[18, 19, 20] ['▁손', '님', '이']\n",
      "[21, 22] ['▁많', '아']\n",
      "[23] ['▁첫']\n",
      "[24, 25] ['▁번', '에']\n",
      "[26, 27] ['▁삼', '십']\n",
      "[28] ['▁전']\n",
      "[29, 30, 31] ['▁둘', '째', '번']\n",
      "[32, 33] ['▁오', '십']\n",
      "[34] ['▁전']\n",
      "[35, 36, 37] ['▁오', '랜', '만에']\n",
      "[38, 39, 40] ['▁받아', '보', '는']\n",
      "[41] ['▁십']\n",
      "[42, 43, 44] ['▁전', '짜', '리']\n",
      "[45, 46, 47] ['▁백', '통', '화']\n",
      "[48, 49, 50] ['▁서', '푼', '에']\n",
      "[52, 53, 54] ['▁손', '바', '닥']\n",
      "[55, 56] ['▁위', '엔']\n",
      "[57, 58, 59] ['▁기', '쁨', '의']\n",
      "[60, 61] ['▁눈', '물이']\n",
      "[62, 63] ['▁흘', '러']\n",
      "[64, 65, 66] ['▁컬', '컬', '한']\n",
      "[67, 68] ['▁목', '에']\n",
      "[69, 70] ['▁모', '주']\n",
      "[71, 72, 73] ['▁한', '잔', '을']\n",
      "[74, 75] ['▁적', '셔']\n",
      "[76] ['▁몇']\n",
      "[77] ['▁달']\n",
      "[78] ['▁포']\n",
      "[79, 80] ['▁전', '부터']\n",
      "[81, 82, 83, 84] ['▁콜', '록', '거', '리는']\n",
      "[85] ['▁아내']\n",
      "[86, 87] ['▁생각', '에']\n",
      "[88, 89, 90] ['▁그', '토', '록']\n",
      "[91, 92] ['▁먹', '고']\n",
      "[93, 94, 95] ['▁싶', '다', '던']\n",
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '[MASK]', '[MASK]', '▁삼', '십', '▁전', '▁둘', '째', '번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '터', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '[MASK]', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘', '러', '[MASK]', '[MASK]', '[MASK]', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "import copy, random\n",
    "\n",
    "# 띄어쓰기 단위로 mask하기 위해서 index 분할\n",
    "cand_idx = []  # word 단위의 index array\n",
    "for (i, token) in enumerate(tokens_org):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "        continue\n",
    "    if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "        cand_idx[-1].append(i)\n",
    "    else:\n",
    "        cand_idx.append([i])\n",
    "        \n",
    "# 결과확인\n",
    "for cand in cand_idx:\n",
    "    print(cand, [tokens_org[i] for i in cand])\n",
    "    \n",
    "# random mask를 위해서 index 순서를 섞음\n",
    "random.shuffle(cand_idx)\n",
    "cand_idx\n",
    "\n",
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "mask_lms = []  # mask 된 값\n",
    "for index_set in cand_idx:\n",
    "    if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "          break\n",
    "    if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "          continue\n",
    "    dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "    for index in index_set:\n",
    "        masked_token = None\n",
    "        if dice < 0.8:  # 80% replace with [MASK]\n",
    "            masked_token = \"[MASK]\"\n",
    "        elif dice < 0.9: # 10% keep original\n",
    "            masked_token = tokens[index]\n",
    "        else:  # 10% random word\n",
    "            masked_token = random.choice(vocab_list)\n",
    "        mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "        tokens[index] = masked_token\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53db6ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_idx   : [23, 24, 25, 32, 33, 41, 57, 58, 59, 64, 65, 66, 79, 80]\n",
      "mask_label : ['▁첫', '▁번', '에', '▁오', '십', '▁십', '▁기', '쁨', '의', '▁컬', '컬', '한', '▁전', '부터']\n"
     ]
    }
   ],
   "source": [
    "# 순서 정렬 및 mask_idx, mask_label 생성\n",
    "mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4802dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 띄어쓰기 단위로 mask하기 위해서 index 분할\n",
    "    cand_idx = []\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "\n",
    "    # random mask를 위해서 순서를 섞음\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    tokens_masked = copy.deepcopy(tokens)\n",
    "    mask_lms = []\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:\n",
    "            continue\n",
    "        dice = random.random()\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens_masked[index] = masked_token\n",
    "\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "    return tokens_masked, mask_idx, mask_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a34800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '[MASK]', '[MASK]', '[MASK]', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '喬', '輿', '낱', '[SEP]', '▁손', '바', '닥', '[MASK]', '[MASK]', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '[MASK]', '[MASK]', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '[MASK]', '[MASK]', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [21, 22, 23, 48, 49, 50, 55, 56, 62, 63, 69, 70, 86, 87]\n",
      "mask_label : ['▁많', '아', '▁첫', '▁서', '푼', '에', '▁위', '엔', '▁흘', '러', '▁모', '주', '▁생각', '에']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ca156",
   "metadata": {},
   "source": [
    "## 데이터 전처리 (2) NSP pair 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "993f9244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'],\n",
       " ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'],\n",
       " ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\"\n",
    "\n",
    "# 줄 단위로 tokenize\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "doc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6cab4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "tokens_a: 50 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에']\n",
      "tokens_b: 12 ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "tokens_a: 55 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라']\n",
      "tokens_b: 16 ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "tokens_a: 56 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져']\n",
      "tokens_b: 17 ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "tokens_a: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "tokens_b: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8\n",
    "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3\n",
    "\n",
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우, 학습 데이터를 만듭니다. \n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        #######################################\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "          \n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2371c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df8de600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "is_next: 0\n",
      "tokens_a: 28 ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "tokens_b: 33 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "is_next: 0\n",
      "tokens_a: 39 ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']\n",
      "tokens_b: 22 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "is_next: 1\n",
      "tokens_a: 17 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던']\n",
      "tokens_b: 44 ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "is_next: 1\n",
      "tokens_a: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "tokens_b: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        #######################################\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0     #False\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1    #True\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "058b3f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "is_next: 0\n",
      "tokens_a: 12 ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "tokens_b: 49 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼']\n",
      "tokens: 64 ['[CLS]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '[MASK]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '▁번', '에', '▁삼', '십', '가로', '[MASK]', '[MASK]', '▁번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '[SEP]']\n",
      "masked index: 9 [25, 26, 27, 36, 41, 42, 43, 45, 46]\n",
      "masked label: 9 ['▁그', '날', '은', '▁첫', '▁전', '▁둘', '째', '▁오', '십']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "is_next: 1\n",
      "tokens_a: 12 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔']\n",
      "tokens_b: 49 ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거']\n",
      "tokens: 64 ['[CLS]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '[SEP]', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '갭', '公', '[SEP]', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '[MASK]', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '[MASK]', '▁난', '▁문', '득', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁목', '소', '리가', '▁거', '[SEP]']\n",
      "masked index: 9 [11, 12, 43, 50, 54, 55, 56, 57, 58]\n",
      "masked label: 9 ['▁적', '셔', '▁살', '▁길', '▁떠', '올', '라', '▁아내', '의']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "is_next: 1\n",
      "tokens_a: 17 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던']\n",
      "tokens_b: 44 ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어']\n",
      "tokens: 64 ['[CLS]', '▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던', '[SEP]', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '[SEP]']\n",
      "masked index: 9 [1, 2, 15, 16, 17, 25, 26, 27, 28]\n",
      "masked label: 9 ['▁오늘', '은', '▁달', '라', '던', '▁일', '찍', '이라', '도']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "is_next: 0\n",
      "tokens_a: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "tokens_b: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "tokens: 25 ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]']\n",
      "segment: 25 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 25 ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]']\n",
      "masked index: 3 [7, 8, 15]\n",
      "masked label: 3 ['▁있', '으면', '▁난']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instances = []\n",
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0    # False\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1   # True\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "\n",
    "        # tokens & segment 생성\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "        segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "        print(\"tokens:\", len(tokens), tokens)\n",
    "        print(\"segment:\", len(segment), segment)\n",
    "        \n",
    "        # mask\n",
    "        tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
    "        print(\"masked tokens:\", len(tokens), tokens)\n",
    "        print(\"masked index:\", len(mask_idx), mask_idx)\n",
    "        print(\"masked label:\", len(mask_label), mask_label)\n",
    "\n",
    "        instance = {\n",
    "            \"tokens\": tokens,\n",
    "            \"segment\": segment,\n",
    "            \"is_next\": is_next,\n",
    "            \"mask_idx\": mask_idx,\n",
    "            \"mask_label\": mask_label\n",
    "        }\n",
    "        instances.append(instance)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38c02177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '[MASK]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '▁번', '에', '▁삼', '십', '가로', '[MASK]', '[MASK]', '▁번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [25, 26, 27, 36, 41, 42, 43, 45, 46], 'mask_label': ['▁그', '날', '은', '▁첫', '▁전', '▁둘', '째', '▁오', '십']}\n",
      "{'tokens': ['[CLS]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '갭', '公', '[SEP]', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '[MASK]', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '[MASK]', '▁난', '▁문', '득', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁목', '소', '리가', '▁거', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [11, 12, 43, 50, 54, 55, 56, 57, 58], 'mask_label': ['▁적', '셔', '▁살', '▁길', '▁떠', '올', '라', '▁아내', '의']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 15, 16, 17, 25, 26, 27, 28], 'mask_label': ['▁오늘', '은', '▁달', '라', '던', '▁일', '찍', '이라', '도']}\n",
      "{'tokens': ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [7, 8, 15], 'mask_label': ['▁있', '으면', '▁난']}\n"
     ]
    }
   ],
   "source": [
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e44a8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])\n",
    "        current_length += len(doc[i])\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            # 50% 확률로 TRUE와 FALSE를 지정\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "\n",
    "            # tokens & segment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "    return instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a70892be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁번', '에', '▁삼', '십', '[MASK]', '▁둘', '째', '介', '▁오', '십', '▁전', '▁오', '랜', '만에', '[MASK]', '[MASK]', '[MASK]', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [5, 8, 15, 16, 17, 36, 37, 38, 39], 'mask_label': ['▁전', '▁번', '▁받아', '보', '는', '▁눈', '물이', '▁흘', '러']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁몇', '▁달', '[MASK]', '▁전', '부터', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '[MASK]', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '[MASK]', '▁난', '▁문', '득', '▁떠', '올', '라', '[SEP]', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 5, 8, 9, 10, 11, 32, 39], 'mask_label': ['▁적', '셔', '▁포', '▁콜', '록', '거', '리는', '▁살', '▁길']}\n",
      "{'tokens': ['[CLS]', '에', '▁있어', '▁달', '라', '던', '▁그리', '도', '[MASK]', '[MASK]', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '[SEP]', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', 'え', '맹', '均', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '[MASK]', '[MASK]', '[MASK]', '▁더', '해', '져', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [2, 8, 9, 43, 44, 45, 57, 58, 59], 'mask_label': ['▁있어', '▁나가', '고', '▁거', '세', '져', '▁', '걱', '정은']}\n",
      "{'tokens': ['[CLS]', '▁난', '[MASK]', '[MASK]', '▁이렇게', '▁살', '[MASK]', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [2, 3, 6], 'mask_label': ['▁맨', '날', '▁수']}\n"
     ]
    }
   ],
   "source": [
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10333ed2",
   "metadata": {},
   "source": [
    "## 데이터 전처리 (3) 데이터셋 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "778f0ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ded239d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 105/3957761 [00:00<11:17, 5845.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 lines : ['▁지', '미', '▁카', '터']\n",
      "['▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카', '터', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '▁~', '▁1981', '년', ')', '이다', '.']\n",
      "['▁그는', '▁2002', '년', '▁말', '▁인', '권', '과', '▁중', '재', '▁역할', '에', '▁대한', '▁공', '로를', '▁인정', '받아', '▁노', '벨', '▁평화', '상을', '▁받', '게', '▁되었다', '.']\n",
      "\n",
      "14 lines : ['▁수학']\n",
      "['▁수학', '(', '數', '學', ',', '▁)', '은', '▁양', ',', '▁구조', ',', '▁공간', ',', '▁변화', ',', '▁미', '적', '분', '▁등의', '▁개념', '을', '▁다루', '는', '▁학', '문', '이다', '.', '▁현대', '▁수학', '은', '▁형식', '▁논', '리를', '▁이용', '해서', '▁공', '리로', '▁구성된', '▁추', '상', '적', '▁구조를', '▁연구', '하는', '▁학', '문', '으로', '▁여겨', '지', '기도', '▁한다', '.', '▁수학', '은', '▁그', '▁구조', '와', '▁발전', '▁과정', '에서는', '▁자연', '과학', '에', '▁속하는', '▁물리', '학을', '▁비롯한', '▁다른', '▁학', '문', '들과', '▁깊', '은', '▁연', '관을', '▁맺', '고', '▁있다', '.', '▁하지만', ',', '▁어느', '▁과학', '의', '▁분야', '들과', '는', '▁달리', ',', '▁자연', '계에서', '▁관측', '되지', '▁않는', '▁개념', '들에', '▁대해서', '까지', '▁이론', '을', '▁일반', '화', '▁및', '▁추', '상', '화', '시', '킬', '▁수', '▁있다는', '▁차', '이가', '▁있다고', '▁한다', '.', '▁수', '학자', '들은', '▁그러', '한', '▁개념', '들에', '▁대해서', '▁추', '측', '을', '▁하고', ',', '▁적', '절', '하게', '▁선택', '된', '▁정의', '와', '▁공', '리', '로부터', '의', '▁엄', '밀', '한', '▁연', '역을', '▁통해', '서', '▁추', '측', '들의', '▁진', '위를', '▁파', '악', '한다', '.']\n",
      "['▁수', '학의', '▁기초', '를', '▁확', '실', '히', '▁세', '우', '기', '▁위해', ',', '▁수', '리', '논', '리', '학과', '▁집합', '론', '이', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범', '주', '론', '이', '▁최근', '에도', '▁발전', '되고', '▁있다', '.', '▁“', '근', '본', '▁위', '기', '”', '라는', '▁말', '은', '▁대', '략', '▁19', '00', '년', '에서', '▁1930', '년', '▁사이에', '▁일어난', ',', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁탐', '구를', '▁상징', '적으로', '▁보여', '주는', '▁말이다', '.', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁몇', '▁가지', '▁의견', '▁불', '일', '치는', '▁오늘날', '에도', '▁계속', '되고', '▁있다', '.', '▁수', '학의', '▁기초', '에', '▁대한', '▁위', '기는', '▁그', '▁당시', '▁수많은', '▁논', '쟁', '에', '▁의해', '▁촉', '발', '되었으며', ',', '▁그', '▁논', '쟁', '에는', '▁칸', '토', '어의', '▁집합', '론', '과', '▁브라', '우', '어', '-', '힐', '베', '르트', '▁논', '쟁', '이', '▁포함', '되었다', '.']\n",
      "\n",
      "4 lines : ['▁수학', '▁상', '수']\n",
      "['▁수학', '에서', '▁상', '수', '란', '▁그', '▁값', '이', '▁변', '하지', '▁않는', '▁불', '변', '량', '으로', ',', '▁변', '수의', '▁반대', '말', '이다', '.', '▁물리', '▁상', '수', '와는', '▁달리', ',', '▁수학', '▁상', '수는', '▁물리', '적', '▁측정', '과는', '▁상', '관', '없이', '▁정의', '된다', '.']\n",
      "['▁특정', '▁수학', '▁상', '수', ',', '▁예를', '▁들', '면', '▁골', '롬', '-', '딕', '맨', '▁상', '수', ',', '▁프랑', '세', '즈', '-', '로', '빈', '슨', '▁상', '수', ',', '▁formula', '_1', ',', '▁레', '비', '▁상', '수', '같은', '▁상', '수는', '▁다른', '▁수학', '상', '수', '▁또는', '▁함수', '와', '▁약', '한', '▁상', '관', '관', '계', '▁또는', '▁강한', '▁상', '관', '관', '계를', '▁갖', '는다', '.']\n",
      "\n",
      "10 lines : ['▁문학']\n",
      "['▁문학', '(', '文', '學', ')', '은', '▁언', '어를', '▁예술', '적', '▁표현', '의', '▁제', '재', '로', '▁삼', '아', '▁새로운', '▁의미', '를', '▁창', '출', '하여', ',', '▁인간', '과', '▁사회', '를', '▁진', '실', '되', '게', '▁묘사', '하는', '▁예술', '의', '▁하', '위', '분', '야', '이다', '.', '▁간', '단', '하게', '▁설명', '하면', ',', '▁언', '어를', '▁통해', '▁인간의', '▁삶', '을', '▁미', '적', '(', '美', '的', ')', '으로', '▁형', '상', '화', '한', '▁것이라고', '▁볼', '▁수', '▁있다', '.', '▁문학', '은', '▁원래', '▁문', '예', '(', '文', '藝', ')', '라고', '▁부', '르는', '▁것이', '▁', '옳', '으며', ',', '▁문', '학을', '▁학', '문', '의', '▁대상', '으로서', '▁탐', '구', '하는', '▁학', '문', '의', '▁명칭', '▁역시', '▁문', '예', '학', '이다', '.', '▁문', '예', '학', '은', '▁음악', '사', '학', ',', '▁미술', '사', '학', '▁등과', '▁함께', '▁예술', '학의', '▁핵', '심', '분', '야', '로서', '▁인', '문', '학의', '▁하', '위', '범', '주에', '▁포함', '된다', '.']\n",
      "['▁반', '영', '론', '적', '▁관', '점에', '▁의한', '▁감', '상은', '▁작품', '을', '▁창', '작', '된', '▁당시', '▁시대', '▁정', '황', '과', '▁연결', '시켜', '▁감', '상', '하는', '▁입', '장', '이고', ',', '▁내', '재', '적', '▁관', '점', '의', '▁감', '상은', '▁작품', '의', '▁형식', ',', '▁내용', '에', '▁국', '한', '하여', '▁감', '상', '하는', '▁것이다', '.', '▁표현', '론', '적', '▁관', '점', '의', '▁감', '상은', '▁작가', '의', '▁전기', '적', '▁사실', '과', '▁작품', '을', '▁연결', '시켜', '▁감', '상', '하는', '▁것이', '고', ',', '▁수용', '론', '적', '▁관', '점', '의', '▁감', '상은', '▁독', '자와', '▁작품', '을', '▁연결', '시켜', '▁감', '상', '하는', '▁것을', '▁말한다', '.']\n",
      "\n",
      "10 lines : ['▁나라', '▁목록']\n",
      "['▁이', '▁문', '서는', '▁나라', '▁목록', '이며', ',', '▁전', '▁세계', '▁20', '6', '개', '▁나라', '의', '▁각', '▁현', '황', '과', '▁주', '권', '▁승', '인', '▁정보를', '▁개', '요', '▁형태로', '▁나', '열', '하고', '▁있다', '.']\n",
      "['▁위', '▁목록', '에', '▁포함', '되지', '▁않은', '▁다음', '▁국가', '는', '▁몬', '테', '비', '데', '오', '▁협', '약', '의', '▁모든', '▁조건', '을', '▁만족', '하지', '▁못', '하거나', ',', '▁자주', '적이고', '▁독립', '적', '임을', '▁주장', '하지', '▁않는', '▁국가', '이다', '.']\n",
      "\n",
      "['▁화학']\n",
      "['▁화학', '(', '化', '學', ',', '▁)', '은', '▁물질', '의', '▁성', '질', ',', '▁조성', ',', '▁구조', ',', '▁변화', '▁및', '▁그', '에', '▁수', '반', '하는', '▁에너', '지의', '▁변', '화를', '▁연구', '하는', '▁자연', '과', '학의', '▁한', '▁분야', '이다', '.', '▁물리', '학', '도', '▁역시', '▁물질', '을', '▁다루', '는', '▁학', '문', '이지만', ',', '▁물리', '학', '이', '▁원', '소', '와', '▁화', '합', '물을', '▁모두', '▁포함한', '▁물', '체의', '▁운동', '과', '▁에너', '지', ',', '▁열', '적', '·', '전', '기', '적', '·', '광', '학적', '·', '기', '계', '적', '▁속', '성을', '▁다루', '고', '▁이러한', '▁현', '상', '으로부터', '▁통일', '된', '▁이론', '을', '▁구축', '하려는', '▁것', '과는', '▁달리', '▁화학', '에서는', '▁물질', '▁자', '체를', '▁연구', '▁대상으로', '▁한다', '.', '▁화학', '은', '▁이미', '▁존재', '하는', '▁물질', '을', '▁이용하여', '▁특', '정한', '▁목', '적', '에', '▁맞', '는', '▁새로운', '▁물질', '을', '▁합', '성', '하는', '▁길', '을', '▁제공', '하며', ',', '▁이는', '▁농', '작', '물의', '▁증', '산', ',', '▁질', '병', '의', '▁치료', '▁및', '▁예', '방', ',', '▁에너', '지', '▁효', '율', '▁증', '대', ',', '▁환경', '오', '염', '▁감소', '▁등', '▁여러', '▁가지', '▁이', '점을', '▁제공', '한다', '.']\n",
      "['▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '해', '낸', '▁화', '합', '물을', '▁뜻', '하였으나', '▁지금', '은', '▁유', '기', '▁화', '합', '물의', '▁범', '위가', '▁크게', '▁넓', '어져', '▁탄', '소', '▁사', '슬', '▁또는', '▁탄', '소', '▁고', '리를', '▁가진', '▁모든', '▁화', '합', '물을', '▁뜻', '한다', '.', '▁유', '기', '화', '학의', '▁오', '랜', '▁관', '심', '사는', '▁유', '기', '▁화', '합', '물의', '▁합', '성', '▁메', '커', '니', '즘', '이다', '.', '▁현', '대에', '▁들어', '서', '▁핵', '자', '기', '▁공', '명', '법', '과', '▁X', '선', '▁결정', '학', '▁등이', '▁개발', '되어', '▁유', '기', '▁화', '합', '물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 위키가 주제별로 잘 나눠지는지 여부 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)  \n",
    "            if 0 < len(doc):\n",
    "                if 0 < count:\n",
    "                    count -= 1\n",
    "                    print(len(doc), \"lines :\", doc[0])\n",
    "                    print(doc[1])\n",
    "                    print(doc[-1])\n",
    "                    print()\n",
    "                else:\n",
    "                    break\n",
    "                doc = []\n",
    "        else:  # 빈 줄이 아니면 doc에 저장\n",
    "            pieces = vocab.encode_as_pieces(line)    \n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        print(doc[0])\n",
    "        print(doc[1])\n",
    "        print(doc[-1])\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28b76b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 105/3957761 [00:00<08:12, 8043.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 21 instances: 10\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '[MASK]', '[MASK]', '[MASK]', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [10, 11, 12, 13, 14, 15, 41, 42, 43], 'mask_label': ['▁합', '성', '섬', '유', '등', '의', '▁화', '합', '물을']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [23, 24, 25, 26, 27, 53, 54, 55, 56], 'mask_label': ['▁유', '기', '화', '학', '에서', '▁화', '합', '물', '은']}\n",
      "\n",
      "doc: 14 instances: 7\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '[MASK]', '[MASK]', '[MASK]', '▁화', '합', '물을', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [38, 39, 40, 44, 45, 46, 47, 48, 49], 'mask_label': ['▁탄', '소로', '▁이루어진', '▁연구', '하는', '▁분', '과', '이다', '.']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '[MASK]', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '[MASK]', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 40, 44, 45, 50, 51, 52, 61, 62], 'mask_label': ['으로', '▁이루어진', '▁연구', '하는', '▁원래', '▁유', '기', '▁추', '출']}\n",
      "\n",
      "doc: 4 instances: 2\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [16, 17, 18, 19, 20, 44, 45, 51, 52], 'mask_label': ['▁고', '분', '자', '물', '질', '▁연구', '하는', '▁유', '기']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁고', '분', '자', '물', '질', '[MASK]', '[MASK]', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 10, 11, 12, 13, 14, 15, 21, 22], 'mask_label': ['으로', '▁합', '성', '섬', '유', '등', '의', '▁등', '도']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '任', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '성', '異', '▁동물', '로부터', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [40, 53, 54, 55, 56, 57, 58, 61, 62], 'mask_label': ['▁이루어진', '▁화', '합', '물', '은', '▁식물', '이나', '▁추', '출']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '[MASK]', '[MASK]', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [23, 24, 25, 26, 27, 38, 39, 51, 52], 'mask_label': ['▁유', '기', '화', '학', '에서', '▁탄', '소로', '▁유', '기']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '[MASK]', '[MASK]', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 46, 47, 48, 49, 51, 52, 57, 58], 'mask_label': ['으로', '▁분', '과', '이다', '.', '▁유', '기', '▁식물', '이나']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [44, 45, 50, 53, 54, 55, 56, 61, 62], 'mask_label': ['▁연구', '하는', '▁원래', '▁화', '합', '물', '은', '▁추', '출']}\n",
      "\n",
      "doc: 31 instances: 15\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '[MASK]', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 23, 24, 25, 26, 27, 40, 51, 52], 'mask_label': ['으로', '▁유', '기', '화', '학', '에서', '▁이루어진', '▁유', '기']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '[MASK]', '[MASK]', '[MASK]', '▁연구', '하는', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [23, 24, 25, 26, 27, 41, 42, 43, 50], 'mask_label': ['▁유', '기', '화', '학', '에서', '▁화', '합', '물을', '▁원래']}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# instance 생성 기능 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "            if 0 < len(doc):\n",
    "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "                # save\n",
    "                print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "                print(instances[0])\n",
    "                print(instances[-1])\n",
    "                print()\n",
    "                doc = []\n",
    "                if 0 < count:  # 테스트를 위해서 부분 처리함\n",
    "                    count -= 1\n",
    "                else:\n",
    "                    break\n",
    "        else:  # doc에 저장\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        instances = create_pretrain_instances(doc, 128)\n",
    "        # save\n",
    "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "        print(instances[0])\n",
    "        print(instances[-1])\n",
    "        print()\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a2e2e6",
   "metadata": {},
   "source": [
    "학습에 필요한 데이터를 로딩하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8e89ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):        # 생성되는 단어 목록이 unknown인 경우는 제거합니다. \n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "                    if 0 < len(doc):\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de0291b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3957761/3957761 [06:26<00:00, 10231.42it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/bert_pretrain/data/bert_pre_train.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f762ef46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918141"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c229f876",
   "metadata": {},
   "source": [
    "np.memmap을 사용한 메모리 사용량 최소화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77642489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 0,\n",
       " 0,\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_seq = 128\n",
    "# [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "max_seq = n_seq - 3\n",
    "\n",
    "# 만약 일반적인 Numpy Array에다 데이터를 로딩한다면 이렇게 되겠지만\n",
    "# enc_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# dec_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# labels_nsp = np.zeros((total,), np.int32)\n",
    "# labels_mlm = np.zeros((total, n_seq), np.int32)\n",
    "\n",
    "# np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "\n",
    "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b2308a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/918141 [00:00<26:59, 566.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '일', '▁~', '[MASK]', '[MASK]', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '[MASK]', '▁1981', '년', ')', '이다', '.', '▁지', '미', '▁카', '터', '는', '[MASK]', '[MASK]', '[MASK]', '▁섬', '터', '▁카운', '티', '▁플', '레', '인', '스', '▁마을', '에서', '▁태어났다', '.', '▁조지', '아', '▁공', '과', '대학교', '를', '▁졸업', '하였다', '.', '▁그', '▁후', '▁해', '군에', '▁들어가', '▁전', '함', '·', '원', '자', '력', '·', '잠', '수', '함', '의', '▁승', '무', '원으로', '▁일', '하였다', '.', '▁195', '3', '년', '▁미국', '▁해군', '▁대', '위로', '▁예', '편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '[MASK]', '▁가', '꿔', '▁많은', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁그의', '▁별', '명이', '▁\"', '땅', '콩', '▁농', '부', '\"', '▁(', 'P', 'e', 'an', 'ut', '▁F', 'ar', 'm', 'er', ')', '로', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁지', '미', '▁카', '터', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [3, 4, 9, 10, 11, 17, 28, 29, 30, 90, 94, 95, 96, 97, 98, 119, 120, 121], 'mask_label': ['▁)', '는', '▁3', '9', '번째', '▁~', '▁조지', '아', '주', '▁등을', '▁돈', '을', '▁벌', '었다', '.', '▁알려', '졌다', '.']}\n",
      "enc_token: [5, 3636, 203, 6, 6, 1114, 3731, 788, 243, 49, 3639, 796, 663, 1647, 3689, 3689, 3632, 6, 3008, 3632, 3623, 16, 3606, 18, 3693, 207, 3721, 3609, 6, 6, 6, 630, 3721, 3565, 3842, 429, 3747, 3635, 3633, 1369, 10, 1605, 3606, 1755, 3637, 41, 3651, 830, 3631, 1135, 52, 3606, 13, 81, 87, 1501, 2247, 25, 3786, 3880, 3674, 3638, 3820, 3880, 4203, 3643, 3786, 3608, 249, 3732, 1232, 33, 52, 3606, 479, 3659, 3632, 243, 2780, 14, 1509, 168, 3884, 414, 165, 1697, 4297, 3880, 3710, 3690, 6, 21, 5014, 399, 6, 6, 6, 6, 6, 307, 587, 931, 103, 4320, 4297, 613, 3645, 3725, 98, 3885, 3663, 256, 2543, 309, 337, 3742, 181, 3623, 3610, 6, 6, 6, 4, 18, 3693, 207, 3721, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0  241 3609    0    0    0    0   49 3639  796    0    0\n",
      "    0    0    0  203    0    0    0    0    0    0    0    0    0    0\n",
      " 1755 3637 3653    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0  593    0    0    0 1927 3614  813   17\n",
      " 3606    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0  489  376 3606    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '아', '▁주', '▁상', '원', '▁의원', '▁선거', '에서', '▁낙', '선', '하나', '▁그', '▁선거', '가', '▁부정', '선거', '▁', '였', '음을', '▁입', '증', '하게', '▁되어', '▁당선', '되고', ',', '▁196', '6', '년', '▁조지', '아', '▁주', '▁지', '사', '▁선거', '에', '▁낙', '선', '하지만', '[MASK]', '[MASK]', '▁조지', '아', '▁주', '▁지', '사를', '▁역임', '했다', '.', '▁대통령', '이', '▁되', '기', '▁전', '▁조지', '아', '주', 'ل', '▁가르', '▁파', '▁두', '번', '▁연', '임', '했으며', ',', '[MASK]', '[MASK]', '▁1975', '년까지', '▁조지', '아', '▁지', '사로', '[MASK]', '[MASK]', '[MASK]', '▁조지', '아', '▁주', '지', '사로', '▁지', '내', '면서', ',', '▁미국', '에', '[MASK]', '▁흑', '인', '▁등', '용', '법을', '▁내', '세', '웠다', '.', '[SEP]', '▁1976', '년', '▁대통령', '▁선거', '에', '▁민주', '당', '▁후보', '로', '▁출', '마', '하여', '▁도', '덕', '주의', '[MASK]', '[MASK]', '▁내', '세', '워', ',', '[MASK]', '[MASK]', '▁누', '르고', '로', '▁달러', 'ゴ', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [39, 40, 57, 58, 59, 66, 67, 74, 75, 76, 88, 114, 115, 120, 121, 124, 125, 126], 'mask_label': ['▁1970', '년', '▁상', '원의', '원을', '▁1971', '년부터', '▁근무', '했다', '.', '▁사는', '▁정책', '으로', '▁포', '드를', '▁당선', '되었다', '.']}\n",
      "enc_token: [5, 3637, 37, 76, 3674, 2378, 822, 10, 1567, 3675, 3294, 13, 822, 3615, 2386, 2163, 3603, 3678, 969, 213, 3936, 173, 607, 2387, 317, 3611, 386, 3680, 3632, 1755, 3637, 37, 18, 3627, 822, 3607, 1567, 3675, 1447, 6, 6, 1755, 3637, 37, 18, 451, 1398, 31, 3606, 663, 3604, 450, 3621, 25, 1755, 3637, 3653, 5400, 2190, 146, 157, 3828, 61, 3780, 530, 3611, 6, 6, 3409, 673, 1755, 3637, 18, 982, 6, 6, 6, 1755, 3637, 37, 3617, 982, 18, 3761, 151, 3611, 243, 3607, 6, 1733, 3635, 50, 3724, 2046, 114, 3699, 1853, 3606, 4, 3306, 3632, 663, 822, 3607, 1114, 3731, 958, 3610, 117, 3681, 54, 75, 4096, 238, 6, 6, 114, 3699, 3971, 3611, 6, 6, 807, 2056, 3610, 2178, 7877, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0 1921 3632    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0   76  955  928    0    0    0    0    0    0 3372  523    0    0\n",
      "    0    0    0    0 2711   31 3606    0    0    0    0    0    0    0\n",
      "    0    0    0    0 3554    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0 1421    9    0    0    0    0  119 1486    0    0 2387   43\n",
      " 3606    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁개발', '을', '▁촉', '구', '했으나', '▁공', '화', '당의', '[MASK]', '[MASK]', '▁무', '산', '되었다', '.', '▁카', '터', '는', '[MASK]', '[MASK]', '[MASK]', '▁이스라엘', '을', '[MASK]', '[MASK]', '[MASK]', '▁캠', '프', '▁데이', '비', '드에서', '▁안', '와', '르', '▁사', '다', '트', '[MASK]', '[MASK]', '▁메', '나', '헴', '▁베', '긴', '▁수상', '과', '▁함께', '▁중', '동', '▁평', '화를', '▁맡았다', '▁캠', '프', '데', '이', '비', '드', '▁협', '정을', '▁체결', '했다', '.', '[SEP]', '▁그러나', '[MASK]', '▁공', '화', '당', '과', '[MASK]', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.', '▁카', '터', '는', '▁1970', '년대', '[MASK]', '[MASK]', '▁대한민국', '▁등', '▁인', '권', '▁후', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 10, 11, 19, 20, 21, 24, 25, 26, 38, 39, 52, 66, 71, 100, 101, 120, 121], 'mask_label': ['지', '▁반', '대로', '▁이집', '트', '와', '▁조정', '하여', ',', '▁대통령', '과', '▁위한', '▁이것은', '▁미국의', '▁소련', '과', '▁후반', '▁당시']}\n",
      "enc_token: [5, 6, 570, 3614, 2270, 3660, 1003, 41, 3690, 1547, 6, 6, 107, 3733, 43, 3606, 207, 3721, 3609, 6, 6, 6, 3426, 3614, 6, 6, 6, 2432, 3728, 965, 3701, 3552, 172, 3672, 3706, 15, 3605, 3684, 6, 6, 334, 3644, 5894, 271, 4106, 1011, 3651, 280, 35, 3665, 232, 934, 1896, 2432, 3728, 3743, 3604, 3701, 3688, 617, 666, 2525, 31, 3606, 4, 330, 6, 41, 3690, 3731, 3651, 6, 2670, 3635, 164, 1314, 141, 3727, 3614, 1213, 4181, 3605, 3606, 2995, 3632, 456, 3935, 3715, 10, 230, 3650, 2714, 2793, 3683, 3834, 9, 1435, 2521, 3606, 276, 1302, 3651, 30, 3626, 3758, 2835, 107, 3621, 1956, 617, 1824, 53, 3635, 31, 3606, 207, 3721, 3609, 1921, 596, 6, 6, 410, 50, 42, 3837, 81, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0 3617    0    0    0    0    0    0    0    0  141  448    0    0\n",
      "    0    0    0    0    0 2703 3684 3672    0    0 3358   54 3611    0\n",
      "    0    0    0    0    0    0    0    0    0    0  663 3651    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  521    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0 1487    0    0    0\n",
      "    0  679    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0 1302 3651    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0 1840  316    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁부', '딪', '혀', '깍', '툴', '▁계열', '도에', '▁완', '전', '철', '수', '▁대신', '▁6', ',000', '명을', '▁감', '축', '하는', '▁데', '▁그', '쳤다', '.', '▁또한', '▁박', '정', '희', '[MASK]', '[MASK]', '▁인', '권', '▁문제', '▁등', '과의', '▁논란', '으로', '▁불', '협', '화', '음을', '▁', '냈', '으나', ',', '▁1979', '년', '▁6', '월', '▁하', '순', ',', '▁대한민국', '을', '▁방문', '하여', '▁관계', '가', '▁다', '소', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁그러나', '▁주', '[MASK]', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '[MASK]', '▁1980', '년', '[MASK]', '[MASK]', '[MASK]', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '[MASK]', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '[MASK]', '▁1980', '년', '▁하계', '▁올림픽', '에', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 6, 7, 8, 9, 29, 30, 61, 62, 63, 67, 81, 84, 85, 86, 99, 121], 'mask_label': ['▁반', '대에', '▁주', '한', '미', '군은', '▁정', '권의', '▁회복', '되었다', '.', '▁이란', '▁이유로', '▁대통령', '▁선거', '에서', '▁결국', '▁인해']}\n",
      "enc_token: [5, 6, 6, 51, 5155, 4185, 6398, 4758, 2440, 1464, 443, 3647, 3924, 3643, 1083, 125, 847, 859, 209, 3916, 38, 189, 13, 1523, 3606, 276, 338, 3649, 4062, 6, 6, 42, 3837, 550, 50, 786, 2408, 9, 128, 4000, 3690, 969, 3603, 4128, 191, 3611, 2995, 3632, 125, 3669, 27, 3953, 3611, 410, 3614, 2017, 54, 704, 3615, 29, 3695, 6, 6, 6, 4, 330, 37, 6, 243, 2630, 3715, 42, 3899, 636, 10, 42, 3899, 73, 3778, 1579, 3631, 6, 1640, 3632, 6, 6, 6, 41, 3690, 1547, 194, 4051, 3688, 1169, 3810, 958, 113, 3603, 3951, 6, 174, 2087, 1579, 31, 3606, 276, 273, 3621, 150, 329, 870, 3720, 1302, 3608, 26, 2986, 3740, 1323, 3232, 636, 9, 6, 1640, 3632, 2219, 779, 3607, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0  141  867    0    0    0   37 3619 3693  941    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0   36 2649    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0 3332   43 3606    0    0    0 3290    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0 1827    0    0\n",
      "  663  822   10    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0  875    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0  751    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '한', '▁뒤', '▁민주', '주의', '▁실', '현', '을', '▁위해', '▁제', '▁3', '세', '계의', '[MASK]', '▁감', '시', '▁활동', '▁및', '▁기', '니', '[MASK]', '[MASK]', '[MASK]', '▁의한', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁질', '병', '▁방', '재', '를', '▁위해', '▁힘', '썼', '다', '.', '▁미국의', '▁빈', '곤', '층', '▁지원', '▁활동', ',', '▁사랑', '의', '[MASK]', '[MASK]', '[MASK]', '▁운동', ',', '▁국제', '▁분', '쟁', '▁중', '재', '▁등의', '[MASK]', '[MASK]', '▁했다', '.', '[SEP]', '[MASK]', '[MASK]', '▁~', '▁1980', '년', '▁대한민국의', '▁정치적', '▁격', '변', '기', '[MASK]', '▁대통령', '이었던', '▁그는', '▁이에', '▁대해', '▁애', '매', '한', '▁태', '도를', '▁보', '였고', ',', '▁이는', '▁후에', '▁대한민국', '▁내에서', '▁고', '조', '되는', '▁반', '미', '▁운동', '의', '▁한', '▁원', '인이', '▁', '됐다', '.', '▁10', '월', '▁26', '일', ',', '▁박', '정', '희', '▁대통령', '이', '▁김', '재', '규', '▁중앙', '정보', '부', '장에', '▁의해', '▁살해', '된', '▁것에', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [13, 20, 21, 22, 24, 25, 26, 27, 28, 29, 49, 50, 51, 60, 61, 65, 66, 75], 'mask_label': ['▁선거', '▁벌', '레', '에', '▁드', '라', '쿤', '쿠', '르', '스', '▁집', '짓', '기', '▁활동', '도', '▁1979', '년', '▁당시의']}\n",
      "enc_token: [5, 3619, 339, 1114, 238, 158, 3763, 3614, 231, 30, 49, 3699, 1654, 6, 209, 3630, 375, 228, 24, 3740, 6, 6, 6, 1332, 6, 6, 6, 6, 6, 6, 761, 3893, 95, 3736, 3631, 231, 947, 4444, 3605, 3606, 679, 1412, 4241, 4090, 770, 375, 3611, 1424, 3608, 6, 6, 6, 887, 3611, 605, 147, 3979, 35, 3736, 507, 6, 6, 345, 3606, 4, 6, 6, 203, 1640, 3632, 447, 2843, 1032, 3896, 3621, 6, 663, 1277, 202, 695, 433, 442, 3830, 3619, 227, 701, 47, 2470, 3611, 594, 1140, 410, 3428, 70, 3683, 267, 141, 3693, 887, 3608, 34, 129, 828, 3603, 1027, 3606, 131, 3669, 981, 3636, 3611, 338, 3649, 4062, 663, 3604, 200, 3736, 3965, 782, 2275, 3645, 1312, 355, 2591, 3718, 2057, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0  822\n",
      "    0    0    0    0    0    0  813 3747 3607    0  311 3642 4963 3944\n",
      " 3706 3633    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0  313 4340 3621    0    0    0    0\n",
      "    0    0    0    0  375 3634    0    0    0 2995 3632    0    0    0\n",
      "    0    0    0    0    0 3195    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁미국', '이', '▁북', '핵', '▁위', '기', ',', '▁코', '소', '보', '▁전쟁', ',', '▁이', '라크', '▁전쟁', '과', '▁같이', '▁미국', '이', '▁군사', '적', '▁행', '동을', '[MASK]', '[MASK]', '[MASK]', '▁선택', '하는', '[MASK]', '[MASK]', '▁사고', '를', '▁버', '리고', '▁군사', '적', '▁행', '동을', '[MASK]', '[MASK]', '[MASK]', '▁행', '위에', '[MASK]', '▁깊', '은', '▁유', '감을', '▁표시', '▁하며', '▁미국의', '▁군사', '적', '[MASK]', '[MASK]', '▁강한', '▁반대', '[MASK]', '[MASK]', '▁보', '이고', '▁있다', '.', '[SEP]', '▁특히', '▁국제', '▁분', '쟁', '▁조', '정을', '▁위해', '▁북한', '의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁아이', '티', '의', '▁세', '드', '라스', '▁장', '군', ',', '▁팔', '레', '인', '스타', '인의', '▁하', '마', '스', ',', '▁보', '스', '니아', '의', '▁세르', '비아', '계', '▁정', '권', '▁같이', '[MASK]', '▁정부', '에', '▁대해', '▁협', '상을', '▁거부', '하면서', '▁사', '태', '의', '▁위', '기를', '▁초', '래', '한', '▁인물', '▁및', '▁단', '체를', '▁직접', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [24, 25, 26, 29, 30, 39, 40, 41, 44, 54, 55, 58, 59, 74, 75, 76, 77, 106], 'mask_label': ['▁최', '후', '로', '▁전통', '적', '▁선', '행', '하는', '▁대해', '▁활동', '에', '▁입', '장을', '▁김', '일', '성', ',', '▁미국']}\n",
      "enc_token: [5, 243, 3604, 251, 4173, 45, 3621, 3611, 258, 3695, 3679, 506, 3611, 8, 3553, 506, 3651, 733, 243, 3604, 1250, 3664, 236, 1629, 6, 6, 6, 1715, 38, 6, 6, 1646, 3631, 407, 999, 1250, 3664, 236, 1629, 6, 6, 6, 236, 1157, 6, 1910, 3620, 46, 2196, 2466, 1368, 679, 1250, 3664, 6, 6, 2632, 1216, 6, 6, 47, 458, 28, 3606, 4, 698, 605, 147, 3979, 53, 666, 231, 1876, 3608, 6, 6, 6, 6, 520, 3842, 3608, 74, 3688, 1951, 104, 3729, 3611, 961, 3747, 3635, 936, 692, 27, 3681, 3633, 3611, 47, 3633, 491, 3608, 3189, 852, 3711, 36, 3837, 733, 6, 513, 3607, 433, 617, 460, 2324, 421, 15, 3807, 3608, 45, 333, 192, 3815, 3619, 1178, 228, 164, 1396, 1069, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  130 3713 3610    0\n",
      "    0 1306 3664    0    0    0    0    0    0    0    0   57 3759   38\n",
      "    0    0  433    0    0    0    0    0    0    0    0    0  375 3607\n",
      "    0    0  213  480    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  200 3636 3657 3611    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0  243    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 라인 단위로 처리\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total)):\n",
    "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "        enc_token += [0] * (n_seq - len(enc_token))\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c4dadeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53a14d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128000/128000 [00:23<00:00, 5538.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a8218f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([   5, 3636,  203,    6,    6, 1114, 3731,  788,  243,   49, 3639,\n",
       "          796,  663, 1647, 3689, 3689, 3632,    6, 3008, 3632, 3623,   16,\n",
       "         3606,   18, 3693,  207, 3721, 3609,    6,    6,    6,  630, 3721,\n",
       "         3565, 3842,  429, 3747, 3635, 3633, 1369,   10, 1605, 3606, 1755,\n",
       "         3637,   41, 3651,  830, 3631, 1135,   52, 3606,   13,   81,   87,\n",
       "         1501, 2247,   25, 3786, 3880, 3674, 3638, 3820, 3880, 4203, 3643,\n",
       "         3786, 3608,  249, 3732, 1232,   33,   52, 3606,  479, 3659, 3632,\n",
       "          243, 2780,   14, 1509,  168, 3884,  414,  165, 1697, 4297, 3880,\n",
       "         3710, 3690,    6,   21, 5014,  399,    6,    6,    6,    6,    6,\n",
       "          307,  587,  931,  103, 4320, 4297,  613, 3645, 3725,   98, 3885,\n",
       "         3663,  256, 2543,  309,  337, 3742,  181, 3623, 3610,    6,    6,\n",
       "            6,    4,   18, 3693,  207, 3721,    4], dtype=int32),\n",
       " memmap([   5, 3664,  738,    6,  344, 1720, 3668,   16, 3606,  484,  336,\n",
       "         3608,  341, 1270,  192, 3669,   24, 3879, 3631,  435, 3686, 3611,\n",
       "          192, 3669,   24, 3879, 3608,  252,  628,  192, 3669,  177, 3643,\n",
       "         3672,  553, 3606, 2847,  344, 3993, 1749, 3642, 3710, 3611,    6,\n",
       "            6,    6,    6,  472, 3643,    6,    6, 2959, 3624, 5645, 7543,\n",
       "         6114, 5588,    1, 4234, 3611,  241,  287,  171, 3606,    4,  336,\n",
       "         3608, 2959,  344, 1035, 3667,  228,  344, 1720, 3608,  740, 2243,\n",
       "          344, 1720, 3668, 3604,   37, 3640, 3955, 3614,   84, 3611, 2847,\n",
       "          484,    6,    6,    6,  344, 1720, 3639, 3607, 1664, 3611,  344,\n",
       "         3993, 1438, 3635,   29, 3864, 1711,  344, 2333, 3613, 2983,    6,\n",
       "            6,    6,  344, 2333, 3626, 3631,   14, 3643, 3664,  738,    6,\n",
       "            6,    6,  171, 3606,    6,    6,    4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 0,\n",
       " 0,\n",
       " memmap([   0,    0,    0,  241, 3609,    0,    0,    0,    0,   49, 3639,\n",
       "          796,    0,    0,    0,    0,    0,  203,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0, 1755, 3637, 3653,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  593,    0,    0,    0, 1927, 3614,  813,   17, 3606,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,  489,  376,\n",
       "         3606,    0,    0,    0,    0,    0,    0], dtype=int32),\n",
       " memmap([   0,    0,    0, 2243,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  344,\n",
       "         1035, 3667, 3631,    0,    0,  192, 3669,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,   29, 3864, 3764,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  858,\n",
       "         3710, 3611,    0,    0,    0,    0,    0,    0,    0,    0, 2243,\n",
       "          197,  354,    0,    0,  344, 1035,    0], dtype=int32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45042ace",
   "metadata": {},
   "source": [
    "## BERT 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a0b5e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b575378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n",
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n",
    "\n",
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d3df35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3ea7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30570603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "339ece29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m) # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, self.n_head * self.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3a61f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbcb63ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bed49b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b522fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7845f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eed0228f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8014,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17a07a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2/2 [==============================] - 33s 13ms/step - loss: 9.8335 - nsp_loss: 0.7788 - mlm_loss: 9.0547 - nsp_acc: 0.3000 - mlm_acc: 0.0100\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 8.6081 - nsp_loss: 0.6227 - mlm_loss: 7.9854 - nsp_acc: 0.7000 - mlm_acc: 0.0300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff1149b8760>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 10\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c238c48",
   "metadata": {},
   "source": [
    "## pretrain 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61ba6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34a170ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d39b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c163d32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "157b9698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 4487424     enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8014)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 4,553,728\n",
      "Trainable params: 4,553,728\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e75a4e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 20000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e84c6937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 250s 124ms/step - loss: 19.5697 - nsp_loss: 0.6513 - mlm_loss: 18.9185 - nsp_acc: 0.5900 - mlm_lm_acc: 0.1114\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.11143, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 247s 123ms/step - loss: 17.5077 - nsp_loss: 0.6241 - mlm_loss: 16.8837 - nsp_acc: 0.6175 - mlm_lm_acc: 0.1306\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.11143 to 0.13062, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 247s 124ms/step - loss: 16.6243 - nsp_loss: 0.6169 - mlm_loss: 16.0074 - nsp_acc: 0.6276 - mlm_lm_acc: 0.1416\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.13062 to 0.14161, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 247s 124ms/step - loss: 14.5488 - nsp_loss: 0.6144 - mlm_loss: 13.9343 - nsp_acc: 0.6290 - mlm_lm_acc: 0.1794\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.14161 to 0.17942, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 248s 124ms/step - loss: 13.5747 - nsp_loss: 0.6086 - mlm_loss: 12.9661 - nsp_acc: 0.6365 - mlm_lm_acc: 0.2056\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.17942 to 0.20563, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 247s 124ms/step - loss: 13.0713 - nsp_loss: 0.6034 - mlm_loss: 12.4679 - nsp_acc: 0.6455 - mlm_lm_acc: 0.2209\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.20563 to 0.22087, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 247s 124ms/step - loss: 12.7484 - nsp_loss: 0.5972 - mlm_loss: 12.1512 - nsp_acc: 0.6576 - mlm_lm_acc: 0.2311\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.22087 to 0.23113, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 247s 124ms/step - loss: 12.5342 - nsp_loss: 0.5911 - mlm_loss: 11.9431 - nsp_acc: 0.6690 - mlm_lm_acc: 0.2378\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.23113 to 0.23783, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 247s 124ms/step - loss: 12.4033 - nsp_loss: 0.5862 - mlm_loss: 11.8171 - nsp_acc: 0.6783 - mlm_lm_acc: 0.2421\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.23783 to 0.24206, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 247s 124ms/step - loss: 12.3474 - nsp_loss: 0.5837 - mlm_loss: 11.7637 - nsp_acc: 0.6824 - mlm_lm_acc: 0.2439\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.24206 to 0.24385, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "model_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/models'\n",
    "\n",
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(\n",
    "    pre_train_inputs,  # inputs\n",
    "    pre_train_labels,  # labels\n",
    "    epochs=epochs,  # epochs\n",
    "    batch_size=batch_size,  # batch_size\n",
    "    callbacks=[save_weights]  # callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a5a748",
   "metadata": {},
   "source": [
    "## 프로젝트 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d3c7d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "# plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('nsp_loss', color='tab:blue')\n",
    "ax1.plot(history.history['nsp_loss'], color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "ax2.set_ylabel('mlm_loss', color='tab:red')  # we already handled the x-label with ax1\n",
    "ax2.plot(history.history['mlm_loss'], color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb6d5b",
   "metadata": {},
   "source": [
    "위 그래프에 따르면, 의도한대로 정확도들은 증가하고, loss들은 감소하는 것을 볼 수 있습니다.  \n",
    "또한 급격한 변화 없이 대체로 완만하게 개선되고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b59a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
